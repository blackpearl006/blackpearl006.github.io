<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://blackpearl006.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://blackpearl006.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-05-31T21:28:07+00:00</updated><id>https://blackpearl006.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Multi-Site MRI data Harmonization</title><link href="https://blackpearl006.github.io/blog/2025/multi-site-mri-data-harmonization/" rel="alternate" type="text/html" title="Multi-Site MRI data Harmonization"/><published>2025-05-27T01:05:58+00:00</published><updated>2025-05-27T01:05:58+00:00</updated><id>https://blackpearl006.github.io/blog/2025/multi-site-mri-data-harmonization</id><content type="html" xml:base="https://blackpearl006.github.io/blog/2025/multi-site-mri-data-harmonization/"><![CDATA[<p><em>A article on best practices to download, perform quality control, extract morphometric features, harmonise &amp; normalise</em></p> <h3>Downloading MRI data</h3> <p>Based on your problem statement you can choose the dataset what you would like to work with. Visit <a href="https://blackpearl006.github.io/NeuroDataHub/">https://blackpearl006.github.io/NeuroDataHub/</a> for access 50+ neuroimaging datasets. Download these images along with the relevant metadata.</p> <h3>Pipeline for morphometric features</h3> <h4>BIDS format</h4> <p>Rearrange the dataset into BIDS format. Ensure dataset description json file is present. Use <a href="https://pypi.org/project/dcm2bids/">dcm2bids</a> or similar packages.</p> <h4><strong>MRI-qc</strong></h4> <p>Perform automated quality control testing using the MRI-qc (<a href="https://github.com/nipreps/mriqc">https://github.com/nipreps/mriqc</a>). You will get various quality control parameters and design a inclusion-exclusion criterion based on your requirements. We recommend using at least 4 parameters including SNR, CNR, Coefficient of joint variation, FWHM to determine the quality of the raw image. Discard a raw image if the raw image does not have at least 3 values above the threshold.</p> <h4>Free Surfer</h4> <p>Use the recon-all command from Free Surfer. Based on the MRI image this command can take 1 to 6 hours. This command can be run in parallel assigning a single core to each subject, effectively scaling up as the number of cores in your CPU. Remember to export SUBJECTS_DIR=&quot;$pwd&quot;, else all the output files will be created in some place else. The stats folder in the output directory contains all the morphometry related information. There are 30+ steps / stages in recon-all process and any one of them can fail, this might be a ghost process. So it’s important that you check for these failures. Best practices include, logging the completion / error code for each of the process and using batches to process.</p> <h4>Free Surfer Segmentation qc</h4> <p>After the Free Surfer recon-all finishes, check whether the Free Surfer was able to correctly get the segmentation maps correct. You can either manually inspect the segmentation maps using freeview or qdec (<a href="https://surfer.nmr.mgh.harvard.edu/fswiki/QuickQaQdec">https://surfer.nmr.mgh.harvard.edu/fswiki/QuickQaQdec</a>).</p> <h4>Aggregating the Morphometry features</h4> <p>Using the asegstats2table (<a href="https://surfer.nmr.mgh.harvard.edu/fswiki/asegstats2table">https://surfer.nmr.mgh.harvard.edu/fswiki/asegstats2table</a>) &amp; aparstats2table (<a href="https://surfer.nmr.mgh.harvard.edu/fswiki/aparcstats2table">https://surfer.nmr.mgh.harvard.edu/fswiki/aparcstats2table</a>) aggregate all the features into a single table with Free Surfer based morphometric features as the columns and each subject as Rows.</p> <h4>Data Harmonization</h4> <p>For morphometry related features, combat-based algorithms are preferred. Data Harmonization technique can differ based on your dataset type, cross-section or longitudinal. For cross-sectional data, you can use neuroHarmonise (<a href="https://github.com/rpomponio/neuroHarmonize">https://github.com/rpomponio/neuroHarmonize</a>) and for longitudinal data (<a href="https://github.com/jcbeer/longCombat">https://github.com/jcbeer/longCombat</a>)</p> <h3>Pipeline for 3D Images</h3> <h4>Reorient MR images to Standard</h4> <p>Reorient the Raw image into the standard space using the <a href="https://ftp.nmr.mgh.harvard.edu/pub/dist/freesurfer/tutorial_packages/centos6/fsl_507/doc/wiki/Orientation(20)Explained.html">fslreorient2std</a>command.</p> <h4>N4 Bias Correction</h4> <p>Perform Bias Correction on the Raw images using N4BiasFieldCorrection (<a href="https://github.com/ANTsX/ANTs/wiki/N4BiasFieldCorrection">https://github.com/ANTsX/ANTs/wiki/N4BiasFieldCorrection</a>). This takes multiple cores to run and is computationally expensive.</p> <h4>Skull Stripping</h4> <p>Perform Skull Striping using FSL’s bet2 tool set the -f (fractional intensity parameter based on your data)</p> <h4>Registration</h4> <p>To preserve the structural information, we recommend to use Linear Registration either with 6DOF or 12DOF (effects the volume). Use FSL’s flirt tool to perform registration. If the subjects are above the age of 18 you can use the MNI152 1mm or 2mm template, if the subjects are below this age it is highly recommended to use a age-specific template (<a href="https://nist.mni.mcgill.ca/pediatric-atlases-4-5-18-5y/">https://nist.mni.mcgill.ca/pediatric-atlases-4-5-18-5y/</a>)</p> <h4>Normalization</h4> <p>While working with multi-site data, the voxel value distribution can vary drastically and can effect the data distribution. White Stripe Normalization (<a href="https://doi.org/10.1016/j.nicl.2014.08.008">https://doi.org/10.1016/j.nicl.2014.08.008</a>) takes care of this difference and brings all the MRI scans to a same voxel distribution and assigns simialr tissues with the same value.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=67816bfc5596" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">Understanding Linear Algebra - Part 3</title><link href="https://blackpearl006.github.io/blog/2025/determinant/" rel="alternate" type="text/html" title="Understanding Linear Algebra - Part 3"/><published>2025-05-10T00:00:00+00:00</published><updated>2025-05-10T00:00:00+00:00</updated><id>https://blackpearl006.github.io/blog/2025/determinant</id><content type="html" xml:base="https://blackpearl006.github.io/blog/2025/determinant/"><![CDATA[<p>Determinants are a fundamental concept in linear algebra, providing insights into matrix properties, linear transformations, and system solvability. In this post, we will explore the <strong>8 key properties of determinants</strong>, their significance, and examples to illustrate each property.</p> <hr/> <h2 id="property-1-determinant-of-the-identity-matrix">Property 1: Determinant of the Identity Matrix</h2> <p>The determinant of the identity matrix ( I ) is always 1, regardless of its size.</p> <h3 id="example">Example</h3> <p>For a 3x3 identity matrix:</p> \[I = \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}\] <p>The determinant is:</p> \[\text{det}(I) = 1\] <p>This property reflects that the identity matrix does not scale or distort space.</p> <hr/> <h2 id="property-2-row-exchange-and-sign-reversal">Property 2: Row Exchange and Sign Reversal</h2> <p>If two rows (or columns) of a matrix are exchanged, the determinant changes its sign.</p> <h3 id="example-1">Example</h3> <p>Consider the matrix:</p> \[A = \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{bmatrix}\] <p>The determinant is:</p> \[\text{det}(A) = (1)(4) - (2)(3) = -2\] <p>If we exchange the rows:</p> \[A' = \begin{bmatrix} 3 &amp; 4 \\ 1 &amp; 2 \end{bmatrix}\] <p>The determinant becomes:</p> \[\text{det}(A') = (3)(2) - (4)(1) = 2\] <p>The sign has reversed.</p> <hr/> <h2 id="property-3-linear-transformations">Property 3: Linear Transformations</h2> <p>If a row (or column) of a matrix is multiplied by a constant ( k ), the determinant is also multiplied by ( k ). However, adding a multiple of one row to another does not change the determinant.</p> <h3 id="example-1-multiplying-a-row-by-a-constant">Example 1: Multiplying a Row by a Constant</h3> <p>For the matrix:</p> \[A = \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{bmatrix}\] <p>If we multiply the first row by 2:</p> \[A' = \begin{bmatrix} 2 &amp; 4 \\ 3 &amp; 4 \end{bmatrix}\] <p>The determinant becomes:</p> \[\text{det}(A') = 2 \cdot \text{det}(A) = 2 \cdot (-2) = -4\] <h3 id="example-2-adding-a-multiple-of-one-row-to-another">Example 2: Adding a Multiple of One Row to Another</h3> <p>If we add 3 times the first row to the second row:</p> \[A'' = \begin{bmatrix} 1 &amp; 2 \\ 6 &amp; 10 \end{bmatrix}\] <p>The determinant remains unchanged:</p> \[\text{det}(A'') = \text{det}(A) = -2\] <hr/> <h2 id="property-4-equal-rows-or-columns">Property 4: Equal Rows or Columns</h2> <p>If two rows (or columns) of a matrix are identical, the determinant is 0.</p> <h3 id="example-2">Example</h3> <p>For the matrix:</p> \[A = \begin{bmatrix} 1 &amp; 2 \\ 1 &amp; 2 \end{bmatrix}\] <p>The determinant is:</p> \[\text{det}(A) = (1)(2) - (2)(1) = 0\] <p>This property reflects that the matrix is singular and does not represent a valid transformation.</p> <hr/> <h2 id="property-5-row-operations-and-elimination">Property 5: Row Operations and Elimination</h2> <p>Subtracting a multiple of one row from another (as in Gaussian elimination) does not change the determinant.</p> <h3 id="example-3">Example</h3> <p>For the matrix:</p> \[A = \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{bmatrix}\] <p>If we subtract 3 times the first row from the second row:</p> \[A' = \begin{bmatrix} 1 &amp; 2 \\ 0 &amp; -2 \end{bmatrix}\] <p>The determinant remains:</p> \[\text{det}(A') = \text{det}(A) = -2\] <p>This property is crucial in solving systems of equations.</p> <hr/> <h2 id="property-6-row-of-zeroes">Property 6: Row of Zeroes</h2> <p>If a matrix has a row (or column) of zeroes, its determinant is 0.</p> <h3 id="example-4">Example</h3> <p>For the matrix:</p> \[A = \begin{bmatrix} 1 &amp; 2 \\ 0 &amp; 0 \end{bmatrix}\] <p>The determinant is:</p> \[\text{det}(A) = (1)(0) - (2)(0) = 0\] <p>This property indicates that the matrix is singular.</p> <hr/> <h2 id="property-7-upper-triangular-matrices">Property 7: Upper Triangular Matrices</h2> <p>For an upper triangular matrix, the determinant is the product of its diagonal elements.</p> <h3 id="example-5">Example</h3> <p>For the matrix:</p> \[A = \begin{bmatrix} 1 &amp; 2 &amp; 3 \\ 0 &amp; 4 &amp; 5 \\ 0 &amp; 0 &amp; 6 \end{bmatrix}\] <p>The determinant is:</p> \[\text{det}(A) = (1)(4)(6) = 24\] <p>This property simplifies determinant calculations for triangular matrices.</p> <hr/> <h2 id="property-8-singular-matrices">Property 8: Singular Matrices</h2> <p>If the determinant of a matrix is 0, the matrix is singular and non-invertible.</p> <h3 id="example-6">Example</h3> <p>For the matrix:</p> \[A = \begin{bmatrix} 1 &amp; 2 \\ 2 &amp; 4 \end{bmatrix}\] <p>The determinant is:</p> \[\text{det}(A) = (1)(4) - (2)(2) = 0\] <p>Since the determinant is 0, ( A ) is singular and does not have an inverse.</p> <hr/> <p>Determinants are a powerful tool in linear algebra, providing insights into matrix properties, transformations, and system solvability. In the next post, we will explore <strong>eigenvalues</strong>, <strong>eigenvectors</strong>, and their applications in understanding linear transformations.</p>]]></content><author><name>Ninad</name></author><category term="linear-algebra"/><category term="distill"/><summary type="html"><![CDATA[A detailed exploration of the 8 fundamental properties of determinants with examples and visualizations.]]></summary></entry><entry><title type="html">Understanding Linear Algebra - Part 1</title><link href="https://blackpearl006.github.io/blog/2025/linalgebra/" rel="alternate" type="text/html" title="Understanding Linear Algebra - Part 1"/><published>2025-05-09T00:00:00+00:00</published><updated>2025-05-09T00:00:00+00:00</updated><id>https://blackpearl006.github.io/blog/2025/linalgebra</id><content type="html" xml:base="https://blackpearl006.github.io/blog/2025/linalgebra/"><![CDATA[<p>Linear algebra is the foundation of many fields, from computer graphics to machine learning. In this post, we will explore the <strong>row picture</strong>, <strong>column picture</strong>, and <strong>matrix form</strong> of a system of equations. We’ll also dive into the <strong>elimination method</strong> with detailed examples.</p> <hr/> <h2 id="row-picture-column-picture-and-matrix-form">Row Picture, Column Picture, and Matrix Form</h2> <p>A system of linear equations can be visualized in three ways:</p> <ol> <li><strong>Row Picture</strong>: Each equation is represented as a line in a 2D plane.</li> <li><strong>Column Picture</strong>: The system is expressed as a combination of column vectors.</li> <li><strong>Matrix Form</strong>: The system is written compactly as a matrix equation.</li> </ol> <h3 id="example-system-of-equations">Example System of Equations</h3> <p>Consider the system:</p> \[\begin{aligned} x + 2y &amp;= 5 \\ 3x + 4y &amp;= 6 \end{aligned}\] <hr/> <h3 id="row-picture">Row Picture</h3> <p>In the row picture, each equation is a line in the 2D plane. The solution is the intersection of these lines.</p> <pre><code class="language-chartjs">{
  "type": "line",
  "data": {
    "labels": [-10, -5, 0, 5, 10],
    "datasets": [
      {
        "label": "x + 2y = 5",
        "data": [
          { "x": -10, "y": 7.5 },
          { "x": 10, "y": -2.5 }
        ],
        "borderColor": "rgba(75,192,192,1)",
        "fill": false
      },
      {
        "label": "3x + 4y = 6",
        "data": [
          { "x": -10, "y": 9 },
          { "x": 10, "y": -6 }
        ],
        "borderColor": "rgba(255,99,132,1)",
        "fill": false
      }
    ]
  },
  "options": {
    "scales": {
      "x": { "type": "linear", "position": "bottom" },
      "y": { "type": "linear" }
    }
  }
}
</code></pre> <hr/> <h3 id="column-picture">Column Picture</h3> <p>In the column picture, the system is written as:</p> \[x \begin{bmatrix} 1 \\ 3 \end{bmatrix} + y \begin{bmatrix} 2 \\ 4 \end{bmatrix} = \begin{bmatrix} 5 \\ 6 \end{bmatrix}\] <p>This means we are looking for a linear combination of the column vectors that equals the right-hand side vector.</p> <hr/> <h3 id="matrix-form">Matrix Form</h3> <p>The system can also be written compactly as:</p> \[\begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} 5 \\ 6 \end{bmatrix}\] <hr/> <h2 id="elimination-method">Elimination Method</h2> <p>The elimination method transforms the system into an equivalent one that is easier to solve. The goal is to eliminate one variable to solve for the other.</p> <h3 id="example-1-solving-the-system">Example 1: Solving the System</h3> <ol> <li> <p>Start with the system: \(\begin{aligned} x + 2y &amp;= 5 \\ 3x + 4y &amp;= 6 \end{aligned}\)</p> </li> <li> <p>Multiply the first equation by 3: \(3x + 6y = 15\)</p> </li> <li> <p>Subtract the second equation: \((3x + 6y) - (3x + 4y) = 15 - 6 \\ 2y = 9 \implies y = 4.5\)</p> </li> <li> <p>Substitute ( y = 4.5 ) into the first equation: \(x + 2(4.5) = 5 \implies x = -4\)</p> </li> </ol> <p>The solution is ( x = -4, y = 4.5 ).</p> <hr/> <h3 id="example-2-another-system">Example 2: Another System</h3> <p>Solve the system:</p> \[\begin{aligned} 2x + y &amp;= 8 \\ x - y &amp;= 2 \end{aligned}\] <ol> <li> <p>Add the equations: \((2x + y) + (x - y) = 8 + 2 \\ 3x = 10 \implies x = \frac{10}{3}\)</p> </li> <li> <p>Substitute ( x = \frac{10}{3} ) into the second equation: \(\frac{10}{3} - y = 2 \implies y = \frac{10}{3} - 6 \implies y = -\frac{8}{3}\)</p> </li> </ol> <p>The solution is ( x = \frac{10}{3}, y = -\frac{8}{3} ).</p> <hr/> <p>In the next part, we will explore <strong>determinants</strong>, <strong>inverse matrices</strong>, and their applications in solving systems of equations.</p>]]></content><author><name></name></author><category term="mathematics"/><category term="linear-algebra"/><category term="charts"/><summary type="html"><![CDATA[A deep dive into the fundamentals of linear algebra using advanced visualization techniques.]]></summary></entry><entry><title type="html">Understanding Linear Algebra - Part 2</title><link href="https://blackpearl006.github.io/blog/2025/linalgebra2/" rel="alternate" type="text/html" title="Understanding Linear Algebra - Part 2"/><published>2025-05-09T00:00:00+00:00</published><updated>2025-05-09T00:00:00+00:00</updated><id>https://blackpearl006.github.io/blog/2025/linalgebra2</id><content type="html" xml:base="https://blackpearl006.github.io/blog/2025/linalgebra2/"><![CDATA[<p>In the first part of this series, we explored the <strong>row picture</strong>, <strong>column picture</strong>, and <strong>matrix form</strong> of a system of equations, along with the <strong>elimination method</strong>. In this post, we will dive deeper into <strong>determinants</strong>, <strong>inverse matrices</strong>, and their applications in solving systems of equations. We’ll also use interactive visualizations to make these concepts more intuitive.</p> <hr/> <h2 id="determinants">Determinants</h2> <p>The determinant is a scalar value that provides critical information about a matrix, such as whether it is invertible or the volume scaling factor of a transformation.</p> <h3 id="determinant-of-a-2x2-matrix">Determinant of a 2x2 Matrix</h3> <p>For a 2x2 matrix:</p> \[A = \begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix}\] <p>The determinant is calculated as:</p> \[\text{det}(A) = ad - bc\] <h3 id="example-determinant-of-a-matrix">Example: Determinant of a Matrix</h3> <p>Consider the matrix:</p> \[A = \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{bmatrix}\] <p>The determinant is:</p> \[\text{det}(A) = (1)(4) - (2)(3) = 4 - 6 = -2\] <p>Since the determinant is non-zero, the matrix is invertible.</p> <hr/> <h2 id="inverse-matrices">Inverse Matrices</h2> <p>The inverse of a square matrix ( A ) is a matrix ( A^{-1} ) such that:</p> \[A A^{-1} = A^{-1} A = I\] <p>Where ( I ) is the identity matrix.</p> <h3 id="formula-for-the-inverse-of-a-2x2-matrix">Formula for the Inverse of a 2x2 Matrix</h3> <p>For a 2x2 matrix:</p> \[A = \begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix}\] <p>If ( \text{det}(A) \neq 0 ), the inverse is given by:</p> \[A^{-1} = \frac{1}{\text{det}(A)} \begin{bmatrix} d &amp; -b \\ -c &amp; a \end{bmatrix}\] <h3 id="example-inverse-of-a-matrix">Example: Inverse of a Matrix</h3> <p>Using the matrix ( A ) from the previous example:</p> \[A = \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{bmatrix}\] <p>The determinant is ( -2 ). The inverse is:</p> \[A^{-1} = \frac{1}{-2} \begin{bmatrix} 4 &amp; -2 \\ -3 &amp; 1 \end{bmatrix} = \begin{bmatrix} -2 &amp; 1 \\ 1.5 &amp; -0.5 \end{bmatrix}\] <hr/> <h2 id="solving-systems-of-equations-using-inverse-matrices">Solving Systems of Equations Using Inverse Matrices</h2> <p>A system of equations can be written in matrix form as:</p> \[AX = B\] <p>Where ( A ) is the coefficient matrix, ( X ) is the column vector of variables, and ( B ) is the column vector of constants. If ( A ) is invertible, the solution is:</p> \[X = A^{-1}B\] <h3 id="example-solving-a-system-using-the-inverse">Example: Solving a System Using the Inverse</h3> <p>Consider the system:</p> \[\begin{aligned} x + 2y &amp;= 5 \\ 3x + 4y &amp;= 6 \end{aligned}\] <p>In matrix form:</p> \[\begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} 5 \\ 6 \end{bmatrix}\] <p>The inverse of ( A ) is:</p> \[A^{-1} = \begin{bmatrix} -2 &amp; 1 \\ 1.5 &amp; -0.5 \end{bmatrix}\] <p>Multiply ( A^{-1} ) by ( B ):</p> \[X = A^{-1}B = \begin{bmatrix} -2 &amp; 1 \\ 1.5 &amp; -0.5 \end{bmatrix} \begin{bmatrix} 5 \\ 6 \end{bmatrix} = \begin{bmatrix} -4 \\ 4.5 \end{bmatrix}\] <p>The solution is ( x = -4, y = 4.5 ).</p> <hr/> <h2 id="applications-of-determinants-and-inverse-matrices">Applications of Determinants and Inverse Matrices</h2> <h3 id="1-checking-matrix-invertibility">1. <strong>Checking Matrix Invertibility</strong></h3> <p>A matrix is invertible if and only if its determinant is non-zero. This property is crucial in solving systems of equations and performing linear transformations.</p> <h3 id="2-linear-transformations">2. <strong>Linear Transformations</strong></h3> <p>Determinants can be used to measure how a linear transformation scales or flips the space. For example, a determinant of ( -1 ) indicates a reflection.</p> <h3 id="3-solving-real-world-problems">3. <strong>Solving Real-World Problems</strong></h3> <p>Inverse matrices are widely used in fields like computer graphics, physics simulations, and machine learning to solve systems of linear equations efficiently.</p> <hr/> <p>In the next part, we will explore <strong>eigenvalues</strong>, <strong>eigenvectors</strong>, and their significance in linear transformations, along with more interactive visualizations.</p>]]></content><author><name>Ninad</name></author><category term="linear-algebra"/><category term="charts"/><category term="distill"/><summary type="html"><![CDATA[A comprehensive exploration of determinants, inverse matrices, and their applications in solving systems of equations, with interactive visualizations.]]></summary></entry><entry><title type="html">fMRI preprocessing with FSL</title><link href="https://blackpearl006.github.io/blog/2025/fmri-preprocessing-with-fsl/" rel="alternate" type="text/html" title="fMRI preprocessing with FSL"/><published>2025-02-08T07:19:31+00:00</published><updated>2025-02-08T07:19:31+00:00</updated><id>https://blackpearl006.github.io/blog/2025/fmri-preprocessing-with-fsl</id><content type="html" xml:base="https://blackpearl006.github.io/blog/2025/fmri-preprocessing-with-fsl/"><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*tGpisK1VFOLY8U6K85lyIA.png"/></figure> <p>In this blog i will take you through processing fMRI data and make your fMRI data ready to be used in your analysis. We will use FSL’s FEAT tool for achieving the required results. Take a look at the below video to know how we obtain MRI scans.</p> <iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FRb_mdzgw-Jc%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DRb_mdzgw-Jc&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FRb_mdzgw-Jc%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/111137b1079ea49d511d0bee03cdc0a5/href">https://medium.com/media/111137b1079ea49d511d0bee03cdc0a5/href</a></iframe> <p>There are mainly 2 types of fMRI, <strong>task fMRI</strong> and <strong>Resting state fMRI, </strong>the first open corresponds to when the subject is performing some task like breathing, tapping fingers inside the scanner and the latter corresponds to the subject being still and trying not to fall asleep inside the scanner. Both of them have a very different set of analysis to perform and we’ll go through preprocessing both of the fMRI data types. fMRI is intrinsically more noisier so we will need more tools for the preprocessing</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Y6vndsvKeTUMJLmvyg5hdA.png"/></figure> <p>Click the <a href="https://medium.com/@daminininad/mri-preprocessing-using-fsl-383a67e7185">link</a> to reach my tutorial on processing structural MRI</p> <h4>fMRI data</h4> <p>For this blog we will use the fMRI data of a CN (cognitively normal) subject from ADNI dataset, you can access the data , here : github link ,<br/>I have changed the file structure of the data for ease of programming</p> <h4>Conversion</h4> <p>The data that ADNI provides is in .dcm format and FEAT working only with 4D NIFTI (3D+time) and it’s recommended that the images be brain extracted.</p> <p>Utilize tools like dcm2niix for conversion of .dcm to .nifti</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/960/1*3scPZQ2V3dzEsFU3VvvJ-A.gif"/><figcaption>Visualize the data using fsleyes</figcaption></figure> <h4>Brain extraction</h4> <p>you can see there are non brain material in the above gif for better analysis we would like them to be removed,, if we use bet tool on a 4D image then it will take only the first 3D MRI image and return only one brain extracted MRI image.<br/>So we break the fMRI data into structural MRI image for each point in the timestamp and then perform brain extraction and then again put them together to achieve brain</p> <pre>#Extract individual volumes (timepoints) from the 4D dataset<br />fslsplit 002_S_4262_1.nii volumes -t<br /><br />#Perform brain extraction on each volume separately<br />for volume in volumes*.nii.gz; do<br />  base_name=$(basename &quot;$volume&quot; .nii.gz)<br />  bet &quot;$volume&quot; &quot;brain_extracted_$base_name&quot;<br />done<br /><br />#Merge the brain-extracted volumes back into a 4D dataset<br />fslmerge -t out_brain_extracted.nii.gz brain_extracted_*.nii.gz<br /><br />#delete all the intermediate files<br />rm -r brain_extracted*.nii.gz volumes*.nii.gz</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/960/1*gzi_pja_twTBAC7GqQaDfA.gif"/><figcaption>brain extracted fMRI</figcaption></figure> <h3>Resting state fMRI</h3> <p><strong>Fact:</strong> the energy consumption of the various parts of the brains, brains has ~2% body weight but ~20% energy consumption.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/709/1*O3LS5PjdqTzyW0XAGgMTeA.jpeg"/></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/334/1*pvPUtAhcZN3HWY8zsV1ddw.png"/><figcaption>(left)fMRI scan showing regions of the default mode network; this network shows more activity at rest. (right) Resting state fMRI of 2 young individuals with no cognitive deficit but one has genetic risk factor for alzheimer’s disease</figcaption></figure> <p>In task fMRI analysis, we know what to expect / look for ,, ie. there will be some noticeable changes in intensity and we model that using probability distributions or Gaussian linear models, but in Resting State fMRI we don’t know what to expect and there adds noise in every step of the process, and these noise can dominate in the signal we are interested in thus dominate our final results</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*le-v4x9527b4aGbuhmjXDQ.png"/><figcaption>Probable causes for noise in fMRI data</figcaption></figure> <h3>Analysis of resting-state fMRI</h3> <p>Using FSLeyes visualize the high-res anatomical T1w images and it’s corresponding fMRI image</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*NpH3CP1Vw_wBeo7t"/><figcaption>Visualizing structural MRI file inside anat folder</figcaption></figure> <p>Perform brain extraction on the high-res image</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*J9uCX5q8UUy-6HYX"/><figcaption>Output from bet brain extraction tool:<br/>Blue overlay is the brain extracted part f = 0.2</figcaption></figure> <p>The FEATtool is used to preprocess the fMRI, the below demo directs you what to choose the options as before you run the preprocessing. Specifically, this demo shows you how to upload the data into the preprocessing setup tool.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/960/0*HwDTV5inqV28SRI3"/><figcaption>Uploading 4D fMRI data</figcaption></figure> <p>Snippet of the Pre-stats tab in the FEAT toolbox</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*PWpod-GjSU6B4sJ-"/></figure> <p>Demo on what slice time correction means and how it affects the analysis.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/854/0*O8aICDKEI2X6lrxK"/><figcaption>Most MRI scanners use interleaved MRI slice acquisitionFSL’s default is to <strong>not</strong> do slice-timing correction, and to include a <strong>temporal derivative</strong> instead</figcaption></figure> <p>Options to select on the registration tab</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/960/0*QXTx99bUp1Moe-0o"/></figure> <p>Click Go and you will start the analysis, <br/>Below is the summary of the analysis performed by the FEAT tool</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/960/0*orGChohYTfNzXGsu"/></figure> <p>Visualize the output preprocessed fMRI data saved as filtered_func_data.nii.gz</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/960/0*0XQ3vg5mz-hdLFv3"/></figure> <p>Refrences</p> <ol><li>Andy’s Brain Book : <a href="https://andysbrainbook.readthedocs.io/en/latest/fMRI_Short_Course/fMRI_01_DataDownload.html">https://andysbrainbook.readthedocs.io/en/latest/fMRI_Short_Course/fMRI_01_DataDownload.html</a></li></ol> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=89668e78b11b" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">Intro to AI with Neuroimaging Data: A end to end tutorial using PyTorch</title><link href="https://blackpearl006.github.io/blog/2024/intro-to-ai-with-neuroimaging-data-a-end-to-end-tutorial-using-pytorch/" rel="alternate" type="text/html" title="Intro to AI with Neuroimaging Data: A end to end tutorial using PyTorch"/><published>2024-11-13T10:18:27+00:00</published><updated>2024-11-13T10:18:27+00:00</updated><id>https://blackpearl006.github.io/blog/2024/intro-to-ai-with-neuroimaging-data-a-end-to-end-tutorial-using-pytorch</id><content type="html" xml:base="https://blackpearl006.github.io/blog/2024/intro-to-ai-with-neuroimaging-data-a-end-to-end-tutorial-using-pytorch/"><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*FTycrusmzsEmAONYWL_c5Q.png"/><figcaption>Neurolight Logo; AI generated</figcaption></figure> <p>This is a beginner-friendly guide to using artificial intelligence with neuroimaging data. This tutorial takes you step-by-step through the entire process, starting from loading and understanding MRI data, to building and training deep learning models in PyTorch. Along the way, you’ll learn essential AI and machine learning concepts, see how to set up a dataset for training, and explore real-world tasks like classifying brain images. With clear explanations and practical examples, this tutorial is designed to make complex ideas easy to understand, helping you get hands-on experience with AI in healthcare applications and giving you the skills to work on exciting neuroimaging projects from start to finish.</p> <h3>File Structure and Labels</h3> <p>You must be able to uniquely match your file with it’s corresponding entry in the metadata. For this purpose always encode a unique subject id usually provided by the dataset as a part of the filename. For the purpose of this project I have used preprocessed <a href="https://www.humanconnectome.org/study/hcp-young-adult/data-releases">HCP dataset</a>. The NifTi images are named as Sub-100004.nii.gz &amp; Sub-100206.nii.gz etc. All the files are located in a single folder without any sub-directories. The metadata file is Metadata.csv file, where you can map the filename with unique Subject column.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*gQIpRiMsCM4fobRrgqvj4w.png"/><figcaption>Sample metadata.csv</figcaption></figure> <h4>Importing necessary libraries</h4> <pre>import os<br />import logging<br />import torch<br />import torch.nn as nn<br />import torch.nn.functional as F<br />from sklearn.model_selection import train_test_split<br />import numpy as np<br />import logging<br />import datetime<br />import nibabel as nib<br />import pandas as pd<br />import seaborn as sns<br />from sklearn.metrics import confusion_matrix, accuracy_score, recall_score<br />import matplotlib.pyplot as plt<br />from sklearn.model_selection import KFold<br />from torch.utils.data import DataLoader, Subset, random_split, Dataset<br />from torch.utils.data import *</pre> <h4>Hyper parameters</h4> <p>Initialing all the required hyper-parameters <strong>forefront</strong>. (Important !!, As you get experience, you’ll understand why !!, For now, Take my word)</p> <pre>VAL_RATIO = 0.2<br />TEST_RATIO = 0.2<br />TRAIN_RATIO = 1 - VAL_RATIO - TEST_RATIO<br />DATA_PARALELL = True<br />DEVICE = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)<br />DATASET = &#39;HCP&#39;<br />PHASE = &#39;TRAIN&#39;<br />K_FOLDS = 5<br /><br />TASK = &#39;classification&#39;<br />ROOT_DIR = &#39;path/to/dataset/HCP&#39;<br />LABEL_DIR=&#39;path/to/metadata/metadata.csv&#39;<br />LOG_DIR = &#39;path/to/scripts/logs&#39;<br />timestamp = datetime.datetime.now().strftime(&#39;%d_%m_%Y_%H_%M_%S&#39;)<br />RESULT_DIR = f&#39;path/to/scripts/results/run_{timestamp}&#39;<br />os.makedirs(RESULT_DIR,exist_ok=True)<br />TEST_BATCH_SIZE = 2<br /><br />BATCH_SIZE=16<br />LEARNING_RATE=0.0001<br />NUM_EPOCHS = 10<br />EARLY_STOPPING_PATIENCE = 5<br /><br />NP_SEED = 42<br />TORCH_SEED = 36</pre> <h4>Setting up the Seed</h4> <pre>np.random.seed(NP_SEED)<br />torch.manual_seed(TORCH_SEED)</pre> <h4><strong>Setting up logger</strong></h4> <pre>def setup_logger(logs_dir=LOG_DIR,dataset=None,phase=None):<br />    os.makedirs(logs_dir, exist_ok=True)<br />    logger = logging.getLogger(&#39;RunLogger&#39;)<br />    logger.setLevel(logging.INFO)<br /><br />    if not logger.hasHandlers():<br />        timestamp = datetime.datetime.now().strftime(&#39;%d_%m_%Y_%H_%M_%S&#39;)<br />        file_handler = logging.FileHandler(os.path.join(logs_dir, f&#39;{phase}_{dataset}_{timestamp}.log&#39;))<br />        formatter = logging.Formatter(&#39;%(asctime)s - %(levelname)s - %(message)s&#39;)<br />        file_handler.setFormatter(formatter)<br />        logger.addHandler(file_handler)<br />    <br />    return logger<br /><br />LOGGER = setup_logger(logs_dir=LOG_DIR,phase=PHASE,dataset=DATASET)</pre> <h3>Dataset Class</h3> <pre>class Fieldmapdata(Dataset):<br />    &quot;&quot;&quot;<br />    Dataset class for loading neuroimaging data and associated labels for classification or regression tasks.<br /><br />    Attributes:<br />        root_dir (str): Path to the directory containing image data files (.nii or .nii.gz).<br />        label_dir (str): Path to the CSV file containing labels for each subject.<br />        task (str): Specifies the task type: classification (Gender) or regression (Age).<br />    &quot;&quot;&quot;<br />    def __init__(self, root_dir, label_dir, task=&#39;classification&#39;):<br />        self.labels_df = self.load_labels(label_dir) #load the labels<br />        self.samples = self.make_dataset(root_dir, task=task) #assign the labels to each file in the root_dir<br /><br />    def __len__(self):<br />        return len(self.samples)<br /><br />    def __getitem__(self, idx):<br />        img_path, label = self.samples[idx]<br />        nifti_data = nib.load(img_path)<br />        data = nifti_data.get_fdata()<br />        image_tensor = torch.tensor(data, dtype=torch.float32).unsqueeze(0) #unsqueeze to add channel dimension<br />        label_tensor = torch.tensor(label, dtype=torch.float32).unsqueeze(0)<br />        return image_tensor, label_tensor<br />       <br />    def make_dataset(self, root_dir, task = None):<br />        samples = []<br />        labels_df = self.labels_df<br />        for root, _, fnames in os.walk(root_dir):<br />            for fname in fnames:<br />                if fname.endswith(&quot;.nii.gz&quot;) or fname.endswith(&quot;.nii&quot;):<br />                    path = os.path.join(root, fname)<br />                    id_ = self.extract_id_from_filename(fname)<br />                    try:<br />                        if task == &#39;regression&#39;: <br />                            label = labels_df[labels_df[&#39;Subject&#39;] == id_][&#39;Age&#39;].iloc[0] <br />                        if task == &#39;classification&#39;: <br />                            label = labels_df[labels_df[&#39;Subject&#39;] == id_][&#39;Gender&#39;].iloc[0]<br />                        samples.append((path, label))<br />                    except:<br />                        continue<br />        return samples<br />    <br />    def extract_id_from_filename(self, fname):<br />        &#39;&#39;&#39;match the filename to key to query in the labels dictionary&#39;&#39;&#39;<br />        fname = fname.replace(&quot;sub-&quot;,&quot;&quot;)<br />        if fname.endswith(&quot;_ad.nii.gz&quot;):<br />            id_ = fname.replace(&quot;_ad.nii.gz&quot;, &quot;&quot;)<br />        elif fname.endswith(&quot;_rd.nii.gz&quot;):<br />            id_ = fname.replace(&quot;_rd.nii.gz&quot;, &quot;&quot;)<br />        elif fname.endswith(&quot;_adc.nii.gz&quot;):<br />            id_ = fname.replace(&quot;_adc.nii.gz&quot;, &quot;&quot;)<br />        elif fname.endswith(&quot;_fa.nii.gz&quot;):<br />            id_ = fname.replace(&quot;_fa.nii.gz&quot;, &quot;&quot;)<br />        elif fname.endswith(&quot;.nii.gz&quot;):<br />            id_ = fname.replace(&quot;.nii.gz&quot;, &quot;&quot;)<br />        return id_<br /><br />    def load_labels(self, label_path):<br />        &#39;&#39;&#39;tip: use astype(required datatype)&#39;&#39;&#39;<br />        df = pd.read_csv(label_path)<br />        df_filtered = df[[&#39;Subject&#39;, &#39;Gender&#39;, &#39;Age&#39;]].copy()<br />        df_filtered[&#39;Gender&#39;] = df_filtered[&#39;Gender&#39;].map({&#39;M&#39;: 0, &#39;F&#39;: 1}).astype(int)<br />        df_filtered[&#39;Age&#39;] = df_filtered[&#39;Age&#39;].apply(lambda x: (int(x.split(&#39;-&#39;)[0]) + int(x.split(&#39;-&#39;)[1])) // 2 if &#39;-&#39; in x else int(x[:-1])).astype(float)<br />        # In HCP-Y age is a bin of 4 years, here i can assigning the average value of the bin range to each subject<br />        df_filtered[&#39;Subject&#39;] = df_filtered[&#39;Subject&#39;].astype(str)<br />        return df_filtered</pre> <blockquote>The Fieldmapsclass is a custom PyTorch dataset designed to load neuroimaging data for classification or regression tasks, handling both image files and associated labels. It expects a directory of images (NIfTI files) and a csv file containing labels for each subject. When instantiated, the class initializes by loading the labels from the csv and preparing a list of samples, where each sample pairs an image path with its corresponding label. This process, managed by the make_dataset function, iterates through the directory of images, retrieves the subject ID from each filename using the extract_id_from_filename method, and matches it to the appropriate label from the csv file. The labels are processed in load_labels, where ‘<strong>Gender</strong>’ is encoded numerically (0 for male and 1 for female), and ‘<strong>Age</strong>’ is computed as an average for any age bins specified in ranges.</blockquote> <blockquote>The __getitem__method loads an image and its corresponding label, converting the image data to a PyTorch tensor with a channel dimension added for compatibility with neural networks. This method allows each sample to be easily accessed by an index, making the class compatible with PyTorch’s DataLoader. This dataset structure provides flexibility to handle both classification (e.g., gender prediction) and regression tasks (e.g., age prediction) using the same base code, simplifying loading and preparing neuroimaging data for deep learning applications.</blockquote> <h4>Create the dataset and split into training and testing</h4> <pre>dataset = Fieldmapdata(root_dir=ROOT_DIR,label_dir=LABEL_DIR,task=TASK)<br />train_dataset, val_dataset, test_dataset = random_split(dataset, [TRAIN_RATIO, VAL_RATIO, TEST_RATIO])</pre> <h4><strong>Dataloader</strong></h4> <pre>train_dl = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)<br />val_dl = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)<br />test_dl = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False)</pre> <h4><strong>Comparing the shape of image at each stage</strong></h4> <pre>RANDOM_IMAGE_PATH = &#39;path/to/dataset/sub-100610.nii.gz&#39;<br />raw_image = nib.load(RANDOM_IMAGE_PATH).get_fdata()<br />train_image = train_dataset.__getitem__(1)[0]<br />train_batch = next(iter(train_dl))<br /><br />print(&#39;Raw image shape&#39;,raw_image.shape,&#39;\n&#39; &#39;Train image shape&#39;, train_image.shape,&#39;\n&#39; &#39;Batch shape&#39;,train_batch[0].shape)</pre> <blockquote>Raw image shape → (91, 109, 91) <br/>Train image shape → torch.Size([1, 91, 109, 91]) <br/>Batch shape → torch.Size([16, 1, 91, 109, 91])</blockquote> <h4>Stratify the labels for classification</h4> <pre>def stratified_split_classification(dataset, test_size):<br />    labels = [dataset[i][-1] for i in range(len(dataset))]  # Assuming label is the last element<br />    # cannot access sample variable for all instances, will thorw AttributeError: &#39;Subset&#39; object has no attribute &#39;samples&#39;<br />    train_indices, test_indices = train_test_split(<br />        range(dataset.__len__()), test_size=test_size, stratify=labels<br />    )<br />    train_ds = Subset(dataset, train_indices)<br />    test_ds = Subset(dataset, test_indices)<br />    return train_ds, test_ds<br /><br />def splitting_data(dataset, TEST_RATIO: float, stratify: bool=False):<br />    if stratify:<br />        train_ds, test_ds = stratified_split_classification(dataset, TEST_RATIO)<br />    else:<br />        train_ds, test_ds = random_split(dataset, [1-TEST_RATIO, TEST_RATIO])<br />    return train_ds, test_ds</pre> <pre>train_and_val_ds, test_ds = stratified_split_classification(dataset, test_size=TEST_RATIO)<br />train_ds, val_ds = stratified_split_classification(train_and_val_ds, test_size=VAL_RATIO)<br />LOGGER.info(&#39;Data split completed !!&#39;)</pre> <pre>dataset_labels = []<br />for ds in [train_ds, val_ds, test_ds]:<br />    labels = []<br />    for i, j in ds:<br />        labels.append(j)<br />    dataset_labels.append(labels)<br />    <br />for num, phase in enumerate([&#39;Train&#39;, &#39;Val&#39;, &#39;Test&#39;]):<br />    Total_subjects = len(dataset_labels[num]), <br />    males_count = len(dataset_labels[num]) - sum(dataset_labels[num])<br />    females_count = sum(dataset_labels[num]), <br />    ratio = (len(dataset_labels[num]) - sum(dataset_labels[num])) / sum(dataset_labels[num])<br />    print(f&#39;Phase {phase}, total: {Total_subjects[0]}, ratio: {ratio}&#39;)</pre> <blockquote>Phase Train, total: 542, ratio: tensor([0.8007])<br/>Phase Val, total: 136, ratio: tensor([0.7895])<br/>Phase Test, total: 170, ratio: tensor([0.7895])</blockquote> <blockquote>The code defines functions to split a dataset into training, validation, and test sets, with the option to stratify the splits based on class labels. stratified_split_classification performs a stratified split, ensuring that the distribution of labels in the train and test sets matches the original dataset’s distribution, which is particularly useful in classification tasks to prevent class imbalance. The splitting_data function uses this stratified approach if stratify=True; otherwise, it performs a random split.</blockquote> <blockquote>After splitting, the code counts and logs the total number of subjects, along with the counts and ratio of males to females, for each phase (Train, Val, Test). Stratification is crucial when there is class imbalance, as it ensures that each split has a similar proportion of classes, helping to prevent biased model training and evaluation results.</blockquote> <pre>train_dl = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)<br />val_dl = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)<br />test_dl = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False)</pre> <h3><strong>Model</strong></h3> <pre>&#39;&#39;&#39;copied from https://github.com/ha-ha-ha-han/UKBiobank_deep_pretrain/blob/master/dp_model/model_files/sfcn.py&#39;&#39;&#39;<br />&#39;&#39;&#39;This code is hardcoded for a specific input shape: ie. [batch_size, 1, 160, 192, 160]&#39;&#39;&#39;<br /><br />class SFCN(nn.Module):<br />    def __init__(self, channel_number=[32, 64, 128, 256, 256, 64], output_dim=1, dropout=True): #default output_dim changed from 40 to 1<br />        super(SFCN, self).__init__()<br />        n_layer = len(channel_number)<br />        self.feature_extractor = nn.Sequential()<br />        for i in range(n_layer):<br />            if i == 0:<br />                in_channel = 1<br />            else:<br />                in_channel = channel_number[i-1]<br />            out_channel = channel_number[i]<br />            if i &lt; n_layer-1:<br />                self.feature_extractor.add_module(&#39;conv_%d&#39; % i,<br />                                                  self.conv_layer(in_channel,<br />                                                                  out_channel,<br />                                                                  maxpool=True,<br />                                                                  kernel_size=3,<br />                                                                  padding=1))<br />            else:<br />                self.feature_extractor.add_module(&#39;conv_%d&#39; % i,<br />                                                  self.conv_layer(in_channel,<br />                                                                  out_channel,<br />                                                                  maxpool=False,<br />                                                                  kernel_size=1,<br />                                                                  padding=0))<br />        self.classifier = nn.Sequential()<br />        ############################## Hard coded ####################################### <br />        #avg_shape = [5, 6, 5] <br />        #self.classifier.add_module(&#39;average_pool&#39;, nn.AvgPool3d(avg_shape))<br />        ############################## Hard coded ####################################### <br /><br />        ############################## Change ####################################### <br />        self.classifier.add_module(&#39;average_pool&#39;, nn.AdaptiveAvgPool3d(1))<br />        ############################## Change ####################################### <br /><br />        if dropout is True:<br />            self.classifier.add_module(&#39;dropout&#39;, nn.Dropout(0.5))<br />        i = n_layer<br />        in_channel = channel_number[-1]<br />        out_channel = output_dim<br />        self.classifier.add_module(&#39;conv_%d&#39; % i,<br />                                   nn.Conv3d(in_channel, out_channel, padding=0, kernel_size=1))<br /><br />    @staticmethod<br />    def conv_layer(in_channel, out_channel, maxpool=True, kernel_size=3, padding=0, maxpool_stride=2):<br />        if maxpool is True:<br />            layer = nn.Sequential(<br />                nn.Conv3d(in_channel, out_channel, padding=padding, kernel_size=kernel_size),<br />                nn.BatchNorm3d(out_channel),<br />                nn.MaxPool3d(2, stride=maxpool_stride),<br />                nn.ReLU(),<br />            )<br />        else:<br />            layer = nn.Sequential(<br />                nn.Conv3d(in_channel, out_channel, padding=padding, kernel_size=kernel_size),<br />                nn.BatchNorm3d(out_channel),<br />                nn.ReLU()<br />            )<br />        return layer<br /><br />    def forward_original (self, x):<br />        out = list()<br />        x_f = self.feature_extractor(x)<br />        x = self.classifier(x_f)<br />        x = F.log_softmax(x, dim=1)<br />        out.append(x)<br />        return out<br />    <br />    def forward(self, x):<br />        &#39;&#39;&#39;Instead of list output datatype i prefer tensor and as we are performing Binary classification / Regression, we will not apply softmax&#39;&#39;&#39;<br />        # print(&#39;In shape&#39;, x.shape)<br />        x_f = self.feature_extractor(x)<br />        # print(&#39;After feature extraction module shape&#39;, x_f.shape)<br />        x = self.classifier(x_f)<br />        # print(&#39;After classification module shape&#39;, x.shape)<br />        x = x.view(x.size(0), -1) #flattening<br />        # print(&#39;After Flattening shape&#39;, x.shape)<br />        return x</pre> <pre>from torchinfo import summary<br />model = SFCN()<br />summary(model=model, input_size=(16,1,91,109,91))</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Ejcav5KF08FaWOtyx41aQw.png"/><figcaption>Detailed view of the SFCN model</figcaption></figure> <h4><strong>Initializing Model, Optimizer &amp; Loss</strong></h4> <pre>model = SFCN().to(DEVICE)<br />optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)<br />criterion = nn.BCEWithLogitsLoss() <br /># criterion2 = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(1.28,dtype=torch.float32)) # Total subjects: 899 Number of males : 395, number of females: 504, pos_weight = 1.2759493670886075<br /># Females are class 1, ratio of class to class 0 --&gt; 504/395 = 1.27</pre> <h4>Simple Training and Validation loop</h4> <pre>train_losses, val_losses = [], []<br />train_accuracies, val_accuracies = [], []<br /><br />for epoch in range(NUM_EPOCHS):<br />    model.train()<br />    train_loss = 0.0<br />    train_total = 0<br />    train_correct = 0<br /><br />    for inputs, labels in train_dl:<br />        inputs = inputs.to(DEVICE)<br />        labels = labels.to(DEVICE)<br />        optimizer.zero_grad()<br />        outputs = model(inputs)<br />        tloss = criterion(outputs, labels)<br />        tloss.backward()<br />        optimizer.step()<br />        train_loss += tloss.item() / inputs.size(0)<br />        probability = torch.sigmoid(outputs) # during inference you have to apply sigmoid<br />        predicted = (probability &gt;= 0.5).float()<br />        train_total += labels.size(0)<br />        train_correct += (predicted == labels).sum().item()<br /><br />    train_accuracy = 100 * train_correct / train_total<br />    train_accuracies.append(train_accuracy)<br />    train_losses.append(train_loss)<br /><br />    model.eval()<br />    val_loss = 0.0<br />    val_total = 0<br />    val_correct = 0<br /><br />    with torch.no_grad():<br />        for inputs, labels in val_dl:<br />            inputs = inputs.to(DEVICE)<br />            labels = labels.to(DEVICE)<br />            outputs = model(inputs)<br />            vloss = criterion(outputs, labels)<br />            val_loss += vloss.item() / inputs.size(0)<br />            probability = torch.sigmoid(outputs)<br />            predicted = (probability &gt;= 0.5).float()<br />            val_total += labels.size(0)<br />            val_correct += (predicted == labels).sum().item()<br /><br />        val_accuracy = 100 * val_correct / val_total<br />        val_accuracies.append(val_accuracy)<br />        val_losses.append(val_loss)<br /><br />        print(f&quot;Epoch [{epoch+1}/{NUM_EPOCHS}]: Train Loss: {train_loss:.4f} Train Accu: {train_accuracy:.2f}% Val Loss: {val_loss:.4f} Val Accu: {val_accuracy:.2f}%&quot;)<br />        LOGGER.info(f&quot;Epoch [{epoch+1}/{NUM_EPOCHS}]: Train Loss: {train_loss:.4f} Train Accu: {train_accuracy:.4f} Val Loss: {val_loss:.4f} Val Accu: {val_accuracy:.4f}&quot;)<br />torch.save(model.state_dict(),&#39;best_model.pt&#39;)</pre> <blockquote>Epoch [1/10]: Train Loss: 1.2264 Train Accu: 66.01% Val Loss: 1.3759 Val Accu: 56.47% <br/>Epoch [2/10]: Train Loss: 0.9750 Train Accu: 80.94% Val Loss: 2.1438 Val Accu: 56.47%</blockquote> <h4>Save the Best model, performing the best on Validation dataset</h4> <pre>best_model = SFCN()<br /># best_model.load_state_dict(&#39;best_model.pt&#39;)<br /><br />#using the above model for now<br />new_state_dict = {}<br />for key, value in model.state_dict().items():<br />    new_key = key.replace(&quot;module.&quot;, &quot;&quot;)<br />    new_state_dict[new_key] = value<br />best_model.load_state_dict(new_state_dict)</pre> <h4>Test Predictions</h4> <pre>best_model.to(DEVICE)<br />best_model.eval()<br />test_loss = 0.0<br />test_total = 0<br />test_correct = 0<br /><br />with torch.no_grad():<br />    all_predicted = []<br />    all_labels = []<br />    for inputs, labels in test_dl:<br />        inputs = inputs.to(DEVICE)<br />        labels = labels.to(DEVICE)<br />        <br />        outputs = best_model(inputs)<br />        te_loss = criterion(outputs, labels)<br />        test_loss += te_loss.item()/inputs.size(0)<br />        probability = torch.sigmoid(outputs)<br />        predicted = (probability &gt;= 0.5).float()<br />        all_predicted.extend(predicted.cpu().numpy().ravel().tolist())<br />        all_labels.extend(labels.cpu().numpy().ravel().tolist())<br />        test_total += labels.size(0)<br />        test_correct += (predicted == labels).sum().item()<br /><br />    test_accuracy = 100 * test_correct / test_total<br /><br />print(f&quot;TEST LOSS: {test_loss}, TEST ACCURACY: {test_accuracy}&quot;)<br />LOGGER.info(f&quot;TEST LOSS: {test_loss}, TEST ACCURACY: {test_accuracy}&quot;)<br />accuracy = accuracy_score(all_labels, all_predicted)<br />sensitivity = recall_score(all_labels, all_predicted)<br />print(f&#39;Accuracy: {accuracy}, Sensitivity (Recall): {sensitivity}&#39;)<br />LOGGER.info(f&#39;Accuracy: {accuracy}, Sensitivity (Recall): {sensitivity}&#39;)<br /><br />cm = confusion_matrix(all_labels, all_predicted)<br />print(&#39;CM: &#39;,cm)<br />LOGGER.info(f&quot;CM : {cm}&quot;)<br />plt.figure(figsize=(8, 6))<br />sns.heatmap(cm, annot=True, fmt=&quot;d&quot;, cmap=&quot;Blues&quot;,xticklabels=[&#39;Male&#39;, &#39;Female&#39;], yticklabels=[&#39;Male&#39;, &#39;Female&#39;])<br />plt.title(&#39;Confusion Matrix&#39;)<br />plt.xlabel(&#39;Predicted&#39;)<br />plt.ylabel(&#39;True&#39;)<br />plt.savefig(&#39;cm.png&#39;)</pre> <blockquote>TEST LOSS: 42.06333679519594, TEST ACCURACY: 55.62130177514793 Accuracy: 0.5562130177514792, <br/>Sensitivity (Recall): 1.0</blockquote> <figure><img alt="" src="https://cdn-images-1.medium.com/max/640/1*tQBxe0QfRrSjSG8PNpE6lg.png"/><figcaption>It’s very baddd right !!!, choosing the best model for testing is important !</figcaption></figure> <h4><strong>K-Fold Cross Validation for Training with early stopping</strong></h4> <pre>kf = KFold(n_splits=K_FOLDS, shuffle=True, random_state=NP_SEED)<br />fold_train_losses, fold_val_losses = [], []<br />fold_train_accuracies, fold_val_accuracies = [], []<br />LOGGER.info(f&quot;{K_FOLDS} FOLDS CROSS VALIATION TARINING STARTED&quot;)<br /><br />for fold, (train_idx, val_idx) in enumerate(kf.split(train_dataset)):<br />    print(f&#39;Fold {fold+1}/{K_FOLDS}&#39;)<br />    LOGGER.info(f&#39;Fold {fold+1}/{K_FOLDS}&#39;)<br />    <br />    train_ds = Subset(train_dataset, train_idx)<br />    val_ds = Subset(train_dataset, val_idx)<br />    <br />    train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)<br />    val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)<br /><br />    model = SFCN().to(DEVICE)<br />    model = nn.DataParallel(model) <br />    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)<br />    <br />    criterion = nn.BCEWithLogitsLoss()<br /><br />    best_val_loss = np.inf<br />    epochs_without_improvement = 0<br /><br />    # Train the model for each fold<br />    for epoch in range(NUM_EPOCHS):<br />        model.train()<br />        train_loss = 0.0<br />        train_total = 0<br />        train_correct = 0<br /><br />        for inputs, labels in train_dl:<br />            inputs = inputs.to(DEVICE)<br />            labels = labels.to(DEVICE)<br />            optimizer.zero_grad()<br />            outputs = model(inputs)<br />            tloss = criterion(outputs, labels)<br />            tloss.backward()<br />            optimizer.step()<br />            train_loss += tloss.item() / inputs.size(0)<br />            probability = torch.sigmoid(outputs)<br />            predicted = (probability &gt;= 0.5).float()<br />            train_total += labels.size(0)<br />            train_correct += (predicted == labels).sum().item()<br /><br />        train_accuracy = 100 * train_correct / train_total<br />        fold_train_accuracies.append(train_accuracy)<br />        fold_train_losses.append(train_loss)<br /><br />        model.eval()<br />        val_loss = 0.0<br />        val_total = 0<br />        val_correct = 0<br /><br />        with torch.no_grad():<br />            for inputs, labels in val_dl:<br />                inputs = inputs.to(DEVICE)<br />                labels = labels.to(DEVICE)<br />                outputs = model(inputs)<br />                vloss = criterion(outputs, labels)<br />                val_loss += vloss.item() / inputs.size(0)<br />                probability = torch.sigmoid(outputs)<br />                predicted = (probability &gt;= 0.5).float()<br />                val_total += labels.size(0)<br />                val_correct += (predicted == labels).sum().item()<br /><br />            val_accuracy = 100 * val_correct / val_total<br />            fold_val_accuracies.append(val_accuracy)<br />            fold_val_losses.append(val_loss)<br /><br />            print(f&quot;Fold [{fold+1}/{K_FOLDS}], Epoch [{epoch+1}/{NUM_EPOCHS}]: Train Loss: {train_loss:.4f} Train Accu: {train_accuracy:.2f}% Val Loss: {val_loss:.4f} Val Accu: {val_accuracy:.2f}%&quot;)<br />            LOGGER.info(f&quot;Fold [{fold+1}/{K_FOLDS}], Epoch [{epoch+1}/{NUM_EPOCHS}]: Train Loss: {train_loss:.4f} Train Accu: {train_accuracy:.2f}% Val Loss: {val_loss:.4f} Val Accu: {val_accuracy:.2f}%&quot;)<br />        <br />        # Early stopping<br />        if epoch &gt; 30:<br />            if val_loss &lt; best_val_loss:<br />                best_val_loss = val_loss<br />                epochs_without_improvement = 0 <br /><br />                new_state_dict = {}<br />                for key, value in model.state_dict().items():<br />                    new_key = key.replace(&quot;module.&quot;, &quot;&quot;)<br />                    new_state_dict[new_key] = value<br />                best_model.load_state_dict(new_state_dict)<br /><br />                # torch.save(model.state_dict(), f&#39;{RESULT_DIR}/model_{fold+1}_epoch{epoch}.pt&#39;) # will create a lot of model and take a lot of space<br />                LOGGER.info(f&quot;Validation loss improved for fold {fold+1} at epoch {epoch+1}&quot;)<br />            else:<br />                epochs_without_improvement += 1<br />                LOGGER.info(f&quot;No improvement in validation loss for {epochs_without_improvement} epochs for fold {fold+1}&quot;)<br /><br />            if epochs_without_improvement &gt;= EARLY_STOPPING_PATIENCE:<br />                print(f&quot;Early stopping triggered for fold {fold+1} at epoch {epoch+1}&quot;)<br />                LOGGER.info(f&quot;Early stopping triggered for fold {fold+1} at epoch {epoch+1}&quot;)<br />                ## save best model before stopping<br />                torch.save(model.state_dict(), f&#39;{RESULT_DIR}/best_model_{fold+1}_epoch{epoch}.pt&#39;)<br />                break<br />    <br />    torch.save(model.state_dict(), f&#39;{RESULT_DIR}/best_model_{fold+1}_epoch{epoch}.pt&#39;) ## save model as best model if the training ends for a fold<br /><br /># Aggregate the results across all folds<br />avg_train_loss = np.mean(fold_train_losses)<br />avg_val_loss = np.mean(fold_val_losses)<br />avg_train_accuracy = np.mean(fold_train_accuracies)<br />avg_val_accuracy = np.mean(fold_val_accuracies)<br /><br />print(f&quot;Avg Train Loss: {avg_train_loss:.4f}, Avg Train Accu: {avg_train_accuracy:.2f}%, Avg Val Loss: {avg_val_loss:.4f}, Avg Val Accu: {avg_val_accuracy:.2f}%&quot;)<br />LOGGER.info(f&quot;Avg Train Loss: {avg_train_loss:.4f}, Avg Train Accu: {avg_train_accuracy:.2f}%, Avg Val Loss: {avg_val_loss:.4f}, Avg Val Accu: {avg_val_accuracy:.2f}%&quot;)</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*mvFbhCMgiEG0-mNbtyLt_w.png"/><figcaption>sample output</figcaption></figure> <p>Test with the Best model obtained across the 5 crosses</p> <pre>best_model = SFCN()<br /><br />best_model_state_dict = torch.load(&#39;path/to/scripts/best_model_2_epoch9.pt&#39;, weights_only=True)<br />new_state_dict = {}<br />for key, value in best_model_state_dict.items():<br />    new_key = key.replace(&quot;module.&quot;, &quot;&quot;)<br />    new_state_dict[new_key] = value<br />best_model.load_state_dict(new_state_dict)<br /><br />best_model.to(DEVICE)<br />best_model.eval()<br />test_loss = 0.0<br />test_total = 0<br />test_correct = 0<br /><br />with torch.no_grad():<br />    all_predicted = []<br />    all_labels = []<br />    for inputs, labels in test_dl:<br />        inputs = inputs.to(DEVICE)<br />        labels = labels.to(DEVICE)<br />        <br />        outputs = best_model(inputs)<br />        te_loss = criterion(outputs, labels)<br />        test_loss += te_loss.item()/inputs.size(0)<br />        probability = torch.sigmoid(outputs)<br />        predicted = (probability &gt;= 0.5).float()<br />        all_predicted.extend(predicted.cpu().numpy().ravel().tolist())<br />        all_labels.extend(labels.cpu().numpy().ravel().tolist())<br />        test_total += labels.size(0)<br />        test_correct += (predicted == labels).sum().item()<br /><br />    test_accuracy = 100 * test_correct / test_total<br /><br />print(f&quot;TEST LOSS: {test_loss}, TEST ACCURACY: {test_accuracy}&quot;)<br />LOGGER.info(f&quot;TEST LOSS: {test_loss}, TEST ACCURACY: {test_accuracy}&quot;)<br />accuracy = accuracy_score(all_labels, all_predicted)<br />sensitivity = recall_score(all_labels, all_predicted)<br />print(f&#39;Accuracy: {accuracy}, Sensitivity (Recall): {sensitivity}&#39;)<br />LOGGER.info(f&#39;Accuracy: {accuracy}, Sensitivity (Recall): {sensitivity}&#39;)<br /><br />cm = confusion_matrix(all_labels, all_predicted)<br />print(&#39;CM: &#39;,cm)<br />LOGGER.info(f&quot;CM : {cm}&quot;)<br />plt.figure(figsize=(8, 6))<br />sns.heatmap(cm, annot=True, fmt=&quot;d&quot;, cmap=&quot;Blues&quot;,xticklabels=[&#39;Male&#39;, &#39;Female&#39;], yticklabels=[&#39;Male&#39;, &#39;Female&#39;])<br />plt.title(&#39;Confusion Matrix&#39;)<br />plt.xlabel(&#39;Predicted&#39;)<br />plt.ylabel(&#39;True&#39;)<br />plt.savefig(&#39;cm.png&#39;)</pre> <blockquote>TEST LOSS: 14.311684928834438, TEST ACCURACY: 89.94082840236686<br/>Accuracy: 0.8994082840236687, Sensitivity (Recall): 0.9354838709677419</blockquote> <figure><img alt="" src="https://cdn-images-1.medium.com/max/640/1*DqZS6_j_nPQSWXJaYlqYoQ.png"/></figure> <p>Link to the <a href="https://github.com/blackpearl006/kaggle-notebooks/blob/main/sex_classification.ipynb">Notebook</a></p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=f941c6ef547a" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">From FMRI to FMRI timeseries</title><link href="https://blackpearl006.github.io/blog/2024/from-fmri-to-fmri-timeseries/" rel="alternate" type="text/html" title="From FMRI to FMRI timeseries"/><published>2024-05-17T10:40:19+00:00</published><updated>2024-05-17T10:40:19+00:00</updated><id>https://blackpearl006.github.io/blog/2024/from-fmri-to-fmri-timeseries</id><content type="html" xml:base="https://blackpearl006.github.io/blog/2024/from-fmri-to-fmri-timeseries/"><![CDATA[<p>An end to end tutorial for extraction of FMR timeseries from ADNI Dataset with codes.</p> <p>By the end of this tutorial/ article, you will be able to extract FMRI timeseries at a desired voxel location. We will extract FMR timeseries at 160 Dosenbach ROIs from 6 classical brain networks ie. Default-mode(34), Frontoparietal(34), Sensorimotor(30), Cingulo-opercular(33), Cerebellum(18), Occipital(21).</p> <p>Link to <a href="https://github.com/blackpearl006/MRI-analysis-using-FSL/tree/main/CN_scripts">scripts</a></p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*rMyZqdvIonn40673"/></figure> <h4>Download the fMRI and sMRI</h4> <p>Download the fMRI data with it’s corresponding sMRI image. For this tutorial, I have used ADNI dataset and have downloded Resting state fMRI and MPRAGE images. For a subject we can have mutiple visits contributing to multiple images. The downloaded images will be a specific file structure and will be in Dicom format.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/596/1*PlxWWxSRXf_JV7gWABsHAw.png"/><figcaption>The file structure from the downloaded ADNI dataset, Here CN corresponds to Cognitively Normal Subjects</figcaption></figure> <h4>Conversion to Dicom to NifTi</h4> <p>Convert the Dicom files to NifTi files using dicom2nifti python library and save the images in another folder which is given by outpath.</p> <pre>#script name: dcmtonii_CN.py<br />import dicom2nifti<br />import os<br /><br />path = &#39;path/CN/ADNI&#39;<br />outpath=&#39;path/CNnifti&#39;<br /><br />for subject in os.listdir(os.path.join(path)):<br />    for modality in os.listdir(os.path.join(path,subject)):<br />        for visit in os.listdir(os.path.join(path,subject,modality)):<br />            for someid in os.listdir(os.path.join(path,subject,modality,visit)):<br />                inpath = os.path.join(path,subject,modality,visit,someid)<br />                out_path = os.path.join(outpath,subject,modality,visit)<br />                print(subject)<br />                if not os.path.exists(out_path):<br />                    os.makedirs(out_path)<br />                dicom2nifti.convert_directory(inpath, out_path, compression=True, reorient=True)</pre> <p>Alternatively you can use dcm2niix command line package to achieve the same results</p> <pre>module load pigz-2.4<br />out_dir=&quot;path/to/CN_nifti&quot;<br />mkdir -p $out_dir<br />dcm2niix -f %i_%t -o $out_dir -z y path/CN/ADNI<br />#change the outputfolder accordingly, this script will store all the nifti <br /># inside the outdir with id followed by the visit time eg: 002_S_0295_20110602075850<br /><br /># check what will be the output folder structure when you do not specify the path</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/558/1*4Mag_OCPGEgXZgEyUru3PQ.png"/><figcaption>The output file structure</figcaption></figure> <h4>Changing the file structure</h4> <p>We will change the file structure and of CNnifti to another folder ‘CNStandard_name’ with subject folder each with 2 NifTI images subjectid_visitdate_f &amp; subjectid_visitdata_s referring functional and structural MRI images respectively. There are multiple files for both the resting state fMRI and sMRI (MPRAGE) and both of the scans are not acquired in the same day. The sMRI is only utilised for linear registration and to obtain a mask of the White Matter and CSF in the brain so we only use the sMRI from the earliest visit by a subject along with fMRI from all visits. The first script handles functional data and the second script handles structural data.</p> <pre>#!/bin/bash<br />#script name : rename.sh<br />base_dir=&quot;/path/CNnifti&quot;<br />out_dir=&quot;path/CNStandard_name&quot;<br />for subject_dir in &quot;$base_dir&quot;/*; do<br />    if [[ -d &quot;$subject_dir&quot; ]]; then<br />        for subject_modality in &quot;$subject_dir&quot;/*; do<br />            if [[ -d &quot;$subject_modality&quot; ]]; then<br />                for subject_visit in &quot;$subject_modality&quot;/*; do<br />                    if [[ -d &quot;$subject_visit&quot; ]]; then<br />                        visit_date=$(basename &quot;$subject_visit&quot;)<br />                        subject_name=$(basename &quot;$subject_dir&quot;)<br />                        nifti_files=(&quot;$subject_visit&quot;/*.nii.gz)<br />                        for nifti_file in &quot;${nifti_files[@]}&quot;; do<br />                            nifti_filename=$(basename &quot;$nifti_file&quot; .nii.gz)<br />                            if [[ $nifti_filename == *&quot;resting&quot;* ]]; then<br />                                newdirname=${subject_name}_${visit_date:0:10}<br />                                mkdir -p &quot;$out_dir/$newdirname&quot;<br />                                trimmed_filename=&quot;${newdirname}_f&quot;<br />                                cp &quot;$nifti_file&quot; &quot;$out_dir/$newdirname/$trimmed_filename.nii.gz&quot;<br />                            fi<br />                        done<br />                    fi<br />                done<br />            fi<br />        done<br />    fi<br />done    </pre> <pre>#!/bin/bash<br />#script name : renames.sh<br />base_dir=&quot;/path/CNnifti&quot;<br />out_dir=&quot;path/CNStandard_name&quot;<br />for subject_dir in &quot;$base_dir&quot;/*; do<br />    if [[ -d &quot;$subject_dir&quot; ]]; then<br />        for subject_modality in &quot;$subject_dir&quot;/*; do<br />            if [[ -d &quot;$subject_modality&quot; ]]; then<br />                for subject_visit in &quot;$subject_modality&quot;/*; do<br />                    if [[ -d &quot;$subject_visit&quot; ]]; then<br />                        visit_date=$(basename &quot;$subject_visit&quot;)<br />                        subject_name=$(basename &quot;$subject_dir&quot;)<br />                        echo $subject_name<br />                        nifti_files=(&quot;$subject_visit&quot;/*.nii.gz)<br />                        for nifti_file in &quot;${nifti_files[@]}&quot;; do<br />                            nifti_filename=$(basename &quot;$nifti_file&quot; .nii.gz)<br />                            if [[ $nifti_filename == *&quot;mprage&quot;* ]]; then<br />                                for allfmr in &quot;$out_dir/$subject_name*&quot;; do<br />                                    for i in $allfmr;do<br />                                        final_subject_name=$(basename &quot;$i&quot;)<br />                                        trimmed_filename=&quot;${final_subject_name}_s&quot;<br />                                        # echo &quot;$i/$trimmed_filename.nii.gz&quot;<br />                                        cp &quot;$nifti_file&quot; &quot;$i/$trimmed_filename.nii.gz&quot;<br />                                    done<br />                                done<br />                                break<br />                            fi<br />                        done<br />                    fi<br />                done<br />            fi<br />        done<br />    fi<br />done    </pre> <h4>Preprocessing Structural MRI images</h4> <p>We will process the sMRI images stored as subjectid_visitdate_s using 2 tools from FSL, {<a href="https://medium.com/@daminininad/mri-preprocessing-using-fsl-383a67e7185">link</a>} <br/>1. robustfov → Reduce FOV of image to remove lower head and neck<br/>2. bet2 → Brain Extraction tool</p> <p>output of the robustfov is subjectid_visitdate_r, which acts as a input to bet2 and produces subjectid_visitdate_sbrain . Lower fractional intensity value (f) is preferred. Set it according to your dataset. {Script}</p> <pre>#!/bin/bash<br />#script name : bet_CN.sh<br />base_dir=&quot;path/CNStandard_name&quot;<br />for subject_dir in &quot;$base_dir&quot;/*; do<br />    if [[ -d &quot;$subject_dir&quot; ]]; then<br />        subject_name=$(basename &quot;$subject_dir&quot;)<br />        # echo &quot;$subject_name&quot;<br />        in_img=&quot;$subject_dir/${subject_name}_s.nii.gz&quot;<br />        robust_img=&quot;$subject_dir/${subject_name}_rbrain&quot; <br />        out_img=&quot;$subject_dir/${subject_name}_sbrain&quot; <br />        robustfov -i $in_img -r $robust_img <br />        bet2 $robust_img $out_img -f 0.3 <br />    fi<br />done</pre> <h4>Preprocessing Functional MRI images</h4> <p>We will process fMRI images stored as subjectid_visitdate_f using First level (1 subject) Preprocessing through FEAT tool from FSL. Through FEAT we will delete initial 10 volumes, use MCFLIRT motion correction, Change Slice timing correction for ADNI3 dataset, Perform brain extraction perform spatial smoothing of 5mm and perform Highpass temporal filtering. We will also register (linear 12 DOF) the fMRI data to it’s corresponding structural image ie. subjectid_visitdate_sbrain and also to the MNI152 space. Using FEAT GUI perform this on one subject and click Save/ Go. You will find a <strong>design.fsf</strong> file in the output.feat directory. This file has all the parameters required to run feat analysis of a particular subject and allows you to run the FEAT analysis on terminal. With few changes in the design file we can run the feat analysis on other subjects on terminal. We will copy this design.fsf file to each subject folder in CNStandard_name and change few parameters in the desgin.fsf through sed command.</p> <blockquote>change 002_S_2010_2011-01-22 to the subjectid_visitdate that you used to run FEAT to get design file. Check few design files manually.</blockquote> <pre>#!/bin/bash<br />#script name : prepare.sh<br />designfile=&#39;path/CNStandard_name/design.fsf&#39; <br /># path to the desgin file<br />base_dir=&quot;path/CNStandard_name&quot;<br />for subject_dir in &quot;$base_dir&quot;/*; do<br />    if [[ -d &quot;$subject_dir&quot; ]]; then<br />        subject_name=$(basename &quot;$subject_dir&quot;)<br />        echo &quot;Processing subject: $subject_name&quot;<br />        # new_designfile=&quot;${subject_dir}/${subject_name}.fsf&quot;<br />        new_designfile=&quot;${subject_dir}/design.fsf&quot;<br />        # echo $new_designfile<br />        cp &quot;$designfile&quot; &quot;$new_designfile&quot;<br />        sed -i &quot;s/002_S_2010_2011-01-22/$subject_name/g&quot; &quot;$new_designfile&quot;<br />        sed -i &quot;s/002_S_2010_2011-01-22_f/${subject_name}_f/g&quot; &quot;$new_designfile&quot;<br />        sed -i &quot;s/002_S_2010_2011-01-22_sbrain/${subject_name}_sbrain/g&quot; &quot;$new_designfile&quot;<br />        echo &quot;Design file copied and modified for $subject_name&quot;<br />    fi<br />done</pre> <p>FEAT analysis provides preprocessed fMRI as<strong> filtered_func_data.nii.gz</strong> file<br/>The script to run the FEAT analysis for each subject is below</p> <pre>#!/bin/bash<br />#script name : feat_CN.sh<br />base_dir=&quot;path/CNStandard_name&quot;<br />for subject_dir in &quot;$base_dir&quot;/*; do<br />    if [[ -d &quot;$subject_dir&quot; ]]; then<br />        subject_name=$(basename &quot;$subject_dir&quot;)<br />        echo &quot;$subject_name&quot;<br />        cd &quot;$subject_dir&quot;<br />        if [ -e *sbrain.nii.gz ] &amp;&amp; [ -e *f.nii.gz ]; then<br />            echo &quot;found $subject_name&quot;<br />            feat design.fsf<br />        else<br />            echo &quot;NOT FOUND $subject_name&quot;<br />        fi<br />    fi<br />done</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/598/1*ROg8FK1Ko3tmGN_282S_hA.png"/><figcaption>The file structure after running FEAT. To get the .feat folder your subject folder should contain _f.nii.gz _sbrain.nii.gz and desgin.fsf. If you are getting only 002_S_2010_2011–01–22.feat, change the subjectname in the prepare.sh file</figcaption></figure> <h4>Processing fMRI</h4> <p>We know the brain consists of White Matter (WM), Grey Matter (GM) and Cerebrospinal fluid (CSF). Gray matter is composed of the soma, which houses cell organelles like mitochondria. Most of the studies consider only gray matter to contribute towards the neural activity, as it has the synaptic junctions which account for the larger part of brain’s energy consumption detected by fMRI.</p> <pre>#!/bin/bash<br /><br />#script name : afterfeat_CN.sh<br />base_dir=&quot;path/CNStandard_name&quot;<br />for subject_dir in &quot;$base_dir&quot;/*.feat; do<br />    if [[ -d &quot;$subject_dir&quot; ]]; then<br />        subject_name=$(basename &quot;$subject_dir&quot;)<br />        echo $subject_name<br />        cd &quot;$base_dir/$subject_name&quot;<br />        # step 1<br />        fast -t 1 -n 3 -H 0.1 -I 4 -l 20.0 -o reg/highres2standard reg/highres2standard.nii.gz<br />        # step 2<br />        convert_xfm -omat reg/invfunc2standard.mat -inverse reg/example_func2standard.mat<br />        flirt -in reg/highres2standard_pve_0.nii.gz -ref filtered_func_data.nii.gz -applyxfm -init reg/invfunc2standard.mat -out reg/highres2standard_csf_reg.nii.gz<br />        flirt -in reg/highres2standard_pve_2.nii.gz -ref filtered_func_data.nii.gz -applyxfm -init reg/invfunc2standard.mat -out reg/highres2standard_wm_reg.nii.gz<br />        # step 3<br />        fslmaths reg/highres2standard_csf_reg.nii.gz -thr 0.95 reg/csf_mask_95.nii.gz<br />        fslmaths reg/highres2standard_wm_reg.nii.gz -thr 0.95 reg/wm_mask_95.nii.gz<br />        # step 4<br />        fslmeants -i filtered_func_data.nii.gz -o csf_with_noise.txt -m reg/csf_mask_95.nii.gz <br />        fslmeants -i filtered_func_data.nii.gz -o wm_with_noise.txt -m reg/wm_mask_95.nii.gz <br />        # step 5<br />        paste csf_with_noise.txt wm_with_noise.txt mc/prefiltered_func_data_mcf.par |tr -d &quot;\t&quot; &gt; paraorig.txt <br />        Text2Vest paraorig.txt paraorig.mat<br />        # step 6<br />        fsl_glm -i filtered_func_data.nii.gz -d paraorig.mat --out_res=res_brain.nii.gz<br />        # step 7<br />        flirt -in res_brain.nii.gz -ref reg/standard.nii.gz -applyxfm -init reg/example_func2standard.mat -out res_brain_std.nii.gz<br />        # step 8<br />        fslmaths reg/standard.nii.gz -mul 0 -Tmin -bin roi_template.nii.gz<br />    fi<br />done</pre> <p>There are 2 spaces that we will be working on Functional space (reference: fMRI → filtered_func_data.nii.gz) and Structural space (reference: sMRI → reg/highres2standard.nii.gz)</p> <ol><li>Brain Matter Segmentation<br/>Using FSL FAST tool we get WM, GM and CSF segmented images from the structural image. (The input is highres2standard.nii.gz which is same as subjectid_visitdate_sbrain.nii.gz registered to MNI152 space)</li><li>Structural to Functional space<br/>Convert WM and CSF Segmented images from structural space to functional space using FLIRT</li><li>Create Mask<br/>By thresholding the amount of CSF / WM is present in the voxel we create binary mask for both the WM and CSF brain matter</li><li>FSL Mean time series (fslmeants)<br/>For both WM and CSF multiply the mask with fMRI and get a mean timeseries for these brain matter</li><li>Paraorigin.mat<br/>Concatenate both these timeseries and to account for the motion correction add mc/prefiltered_func_data_mcf.par and convert that to a .mat file</li><li>Temporal regression<br/>Using this paraorigin.mat perform temporal regression on the fMRI to remove out the contributions from WM and CSF</li><li>MNI space<br/>Register the res_brain to MNI space</li><li>ROI Template<br/>Create a blank canvas with reference to the MNI standard space image where we will plot ROI</li></ol> <h4>Timeseries extraction</h4> <p>You can get MNI co-ordinate for each of the 160 Dosenbach ROIs from the script below.</p> <pre>from nilearn import datasets<br />rois = datasets.fetch_coords_dosenbach_2010()[&#39;rois&#39;]<br />labels = datasets.fetch_coords_dosenbach_2010()[&#39;networks&#39;] #which network do they belong to</pre> <p>We have the MNI coordinates of the ROIs belonging to each brain network in a txt files inside the DoschenbachROI folder.<br/>Based on the MNI coordinates point in the roitemplate image that we have created. Then create a sphere of 5mm radius at this point. Get the mean timeseries across the fMRI using fslmeants. Append the timeseries into a csv file. This csv file will have 1 column and all timeseries from each ROI following once after another.</p> <pre>#!/bin/bash<br /><br />#script name : ts_extract_CN.sh<br />base_dir=&quot;path/CNStandard_name&quot;<br />for i in CB DMN FP OP CO SM; do<br />    file_path=&quot;path/DoschenbachROI/$i.txt&quot;<br />    for subject_dir in &quot;$base_dir&quot;/*.feat; do<br />        if [[ -d &quot;$subject_dir&quot; ]]; then<br />            subject_name=$(basename &quot;$subject_dir&quot;)<br />            echo $subject_name<br />            cd &quot;$base_dir/$subject_name&quot;<br />            subject_name1=$(basename &quot;$subject_dir&quot; .feat)<br />            output_file=&quot;${subject_name1}_${i}.csv&quot;<br />            counter=0<br />            while read line; do<br />                IFS=&#39;,&#39; read -r centerx centery centerz &lt;&lt;&lt; &quot;$line&quot; <br />                counter=$((counter+1))<br />                fslmaths roi_template.nii.gz -mul 0 -add 1 -roi $centerx 1 $centery 1 $centerz 1 0 1 ACCpoint -odt float<br />                fslmaths ACCpoint -kernel sphere 5 -fmean ACCsphere_$counter -odt float<br />                fslmaths ACCsphere_$counter.nii.gz -bin ACCsphere_bin_$counter.nii.gz<br />                roi_timeseries=$(fslmeants -i res_brain_std -m ACCsphere_bin_$counter)<br />                time_series=&quot;$roi_timeseries&quot;<br />                echo &quot;$time_series&quot; &gt;&gt; $output_file<br />            done &lt; $file_path<br />            rm -r ACC*<br />        fi<br />    done<br />done</pre> <blockquote>How can you get timeseries for all ROIs at once ??🤔</blockquote> <p>All the csv files containing the timeseries at at CNStandard_name move them to another folder RAWtime</p> <pre>#!/bin/bash<br />base_dir=&quot;path/CNStandard_name&quot;<br />for i in CB DMN FP OP CO SM; do<br />    out_path=&quot;path/RAWtime/CN/$i&quot;<br />    mkdir -p &quot;$out_path&quot;<br />    for subject_dir in &quot;$base_dir&quot;/*.feat; do<br />        if [[ -d &quot;$subject_dir&quot; ]]; then<br />            subject_name=$(basename &quot;$subject_dir&quot;)<br />            subject_name1=$(basename &quot;$subject_dir&quot; .feat)<br />            echo &quot;$subject_name&quot;<br />            cd &quot;$base_dir/$subject_name&quot; || exit 1  <br />            output_file=&quot;${subject_name1}_${i}.csv&quot;<br />            cp &quot;$output_file&quot; &quot;$out_path&quot;<br />            subject_name1=$(basename &quot;$subject_dir&quot; .feat)<br />            new_file_name=&quot;$subject_name1.csv&quot;<br />            mv &quot;${out_path}/${output_file}&quot; &quot;${out_path}/${new_file_name}&quot;<br />        fi<br />    done<br />done</pre> <p>Reshape the csv files from having one column to have rows as many as the ROIs and columns being the timestamps</p> <pre>import pandas as pd<br />import numpy as np<br />import os<br />networks = {<br />    &#39;CB&#39;: 18,<br />    &#39;CO&#39;:32,<br />    &#39;DMN&#39;:34,<br />    &#39;FP&#39;:21,<br />    &#39;OP&#39;: 22,<br />    &#39;SM&#39;:33<br />}<br />for net,roi in networks.items():<br />    path = f&#39;path/RAWtime/CN/{net}&#39;<br />    if os.path.exists(path):<br />        final_path= f&#39;path/FMRtimeseries/CN/{net}&#39;<br />        os.makedirs(final_path,exist_ok=True)<br />        for csv_path in os.listdir(path=path):<br />            print(csv_path)<br />            try:<br />                df = pd.read_csv(os.path.join(path,csv_path), header=None)<br />                data = np.reshape(df.to_numpy(), (roi, -1))<br />                new_df = pd.DataFrame(data)<br />                new_df.to_csv(f&quot;{final_path}/sub_{csv_path[:-4]}.csv&quot;, header=False,index=False)<br />            except:<br />                print(&#39;Missing&#39;, csv_path)</pre> <p>If you have reached till here give a pat on your back !!</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=9157947b3e35" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">Modified 3D Efficient Net</title><link href="https://blackpearl006.github.io/blog/2023/modified-3d-efficient-net/" rel="alternate" type="text/html" title="Modified 3D Efficient Net"/><published>2023-10-15T08:04:43+00:00</published><updated>2023-10-15T08:04:43+00:00</updated><id>https://blackpearl006.github.io/blog/2023/modified-3d-efficient-net</id><content type="html" xml:base="https://blackpearl006.github.io/blog/2023/modified-3d-efficient-net/"><![CDATA[<p>This blog is the implementation of the <a href="https://ietresearch.onlinelibrary.wiley.com/doi/pdf/10.1049/ipr2.12618">paper</a></p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*fP200xCoAkl_suYM"/><figcaption>Photo by <a href="https://unsplash.com/@v2osk?utm_source=medium&amp;utm_medium=referral">v2osk</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure> <p>Efficient Net was introduced not through the crucible of the Image Net competition but in a paper titled “<a href="https://arxiv.org/pdf/1905.11946.pdf">Efficient Net: Rethinking Model Scaling for Convolutional Neural Networks,</a>” authored by Tan and Le in 2019. This groundbreaking work posed a fundamental question: “<strong>Can we create a model with significantly fewer parameters than the well-established deep CNN&#39;s like Res Net and VGG Net, yet achieve comparable levels of accuracy?</strong>” The answer, as it turned out, was a resounding “<strong>yes</strong>.” However, Efficient Net’s innovation wasn’t solely about reducing parameters; it sought an optimal equilibrium between model depth, width, and resolution. This was achieved through the introduction of a compound scaling method that enabled the creation of efficient yet highly accurate models.</p> <p>In this blog, Using a Modified 3D Efficient Net on Registered and Skull Stripped OASIS MRI images to classify the subjects into Healthy and MCI, and extending the same model to perform Gender classification, Brain Age detection and Brain Age Deficit Prediction using a Double headed model (One Regression and one classification head)</p> <p>OASIS is a publicly available and can be access at <a href="https://www.oasis-brains.org/">link</a>. Raw MRI files is not suitable to be used directly in any analysis as each voxel in the MRI image in different subjects will point to different parts of the brain and non-brain matter which do not have any significance might hinder the model’s capability to learn.Keeping all this in mind I used a custom made pipeline including Brain extraction , bias correction and MNI template registering using FSL and the preprocessed MRI images can be found at the link below :</p> <p><a href="https://github.com/blackpearl006/OASIS_MNI_Registered">GitHub - blackpearl006/OASIS_MNI_Registered</a></p> <p>To know more about the preprocessing steps in detail , read my Blog</p> <p><a href="https://medium.com/@daminininad/mri-preprocessing-using-fsl-383a67e7185">MRI preprocessing using FSL</a></p> <p>I’ve created a custom dataset class designed to handle MRI images stored in the NifTi format using the NiBabel library. This class loads these images as numpy arrays for easy data manipulation. Additionally, it performs a crucial data preprocessing step: normalization, ensuring that the data is consistently scaled for further analysis.</p> <p>Moreover, this dataset class takes on the responsibility of labeling each subject. These labels are assigned based on a key clinical metric: the Clinical Dementia Rating (CDR) value. The resulting labels categorize subjects into two meaningful groups: Non-Demented individuals and those with a diagnosis of Probable Alzheimer’s Disease (AD).</p> <pre>class CustomDataset():<br />    def __init__(self, data_dir, csv_path, num_samples = 200, transform = None):  <br />        self.filelabels = self.load_class_labels(csv_path)<br />        self.file_paths = self.get_file_paths(data_dir)<br />        self.transform = transform<br />        <br />    def __len__(self):<br />        return len(self.file_paths)<br />    <br />    def __getitem__(self, idx):<br />        file_path = self.file_paths[idx]<br />        nifti_data = nib.load(file_path)<br />        data = nifti_data.get_fdata()<br />        preprocessed_data = self.preprocess_data(data)<br />        <br />        preprocessed_tensor = torch.tensor(preprocessed_data, dtype=torch.float32).unsqueeze(0)<br /><br />        file_id = file_path.split(&#39;/&#39;)[-1][0:13]<br />        if self.filelabels is not None:<br />            label = torch.tensor(self.filelabels[file_id], dtype=torch.long)<br />            return preprocessed_tensor, label<br />        else:<br />            return preprocessed_tensor<br />                    <br />    def get_file_paths(self, path):<br />        file_paths = []<br />        scans = os.listdir(path)<br />        scans = [scan for scan in scans if scan[:4] == &#39;OAS1&#39;]<br />        for folder_name in scans:<br />            folder_path = os.path.join(path, folder_name)  # Corrected variable name here<br />            if os.path.isdir(folder_path) :<br />                for file_name in os.listdir(folder_path):<br />                    file_path = os.path.join(folder_path, file_name)<br />                    label = self.filelabels[file_path.split(&#39;/&#39;)[-1][0:13]]<br />                    file_paths.append(file_path)<br />        return file_paths<br />    <br />    <br />    def load_class_labels(self, csv_path):<br />        df = pd.read_csv(csv_path)<br />        class_labels = {}<br />        for _, row in df.iterrows():<br />            id_value = row[&#39;ID&#39;]<br />            cdr_value = row[&#39;CDR&#39;]<br />            class_labels[id_value] = 0 if pd.isna(cdr_value) or float(cdr_value) == 0.0 else 1<br />        return class_labels<br />    <br />    def preprocess_data(self, data):<br />        mean = data.mean()<br />        std = data.std()<br />        normalized_data = (data - mean) / std<br />        return normalized_data<br /><br /><br />csv_path = &#39;path-to-metadata-file/kaggle.csv&#39;<br />data_dir = &#39;path/to/downloaded/OASIS_MNI_Registered&#39;<br />custom_dataset = CustomDataset(data_dir, csv_path)</pre> <p>I’ve divided the dataset into training, validation, and test sets, allocating 70% for training, 20% for validation, and 10% for testing. During model training, I’ve chosen a batch size of 32. This approach helps strike a balance between computational efficiency and effective model training.</p> <h3>The Efficient Net Architecture</h3> <h4>Fundamental Building Block</h4> <p>At the core of this architecture lies a fundamental building block, a straightforward CNN layer enhanced with batch normalization and activated using the SiLU (Sigmoid-weighted Linear Unit) or Swish activation function. This building block forms the basis for our entire implementation.</p> <pre>class CNNBlock3d(nn.Module):<br />    def __init__(self, in_channels, out_channels, kernel_size, stride = 1, padding = 0 , groups=1, act=True, bn=True, bias=False):<br />        super(CNNBlock3d, self).__init__()<br />        self.cnn = nn.Conv3d(in_channels, out_channels, kernel_size, stride, padding, groups=groups, bias=bias) #bias set to False as we are using BatchNorm<br />        <br />        # if groups = in_channels then it is for Depth wise convolutional; For each channel different Convolutional kernel<br />        # very limited change in loss but a very high decrease in number of paramteres<br />        # if groups = 1 : normal_conv kernel of size kernel_size**3<br /><br />        self.bn = nn.BatchNorm3d(out_channels) if bn else nn.Identity() <br />        self.silu = nn.SiLU() if act else nn.Identity() ##SiLU &lt;--&gt; Swish same Thing<br />        # 1 layer in MBConv doesn&#39;t have activation function<br />    <br />    def forward(self, x):<br />        out = self.cnn(x)<br />        out = self.bn(out)<br />        out = self.silu(out)<br />        return out<br />    <br /># return self.silu(self.bn(self.cnn(x)))</pre> <p>The SiLU, also known as the Swish activation function, implemented in our network exhibits a unique characteristic. It permits a small amount of negative gradient to flow within the network, in contrast to the widely popular ReLU activation function, which restricts negative gradient flow. Additionally, although the paper does not explicitly detail batch normalization, we have incorporated it into our model. This addition simplifies the learning process for the model, enhancing its capabilities.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/842/0*WcfMo1jbm7g9Od5X.png"/><figcaption>Difference between ReLU and SiLU (Swish)</figcaption></figure> <h4>The Ranking master</h4> <p>The squeeze excitation layer is used to <strong>compute attention score for each channels,</strong> effectively assigning a value to each channel based on its importance. This process guides us in determining how much emphasis we should place on each channel. Subsequently, the original feature map is then scaled with these channel-specific attention scores, optimizing our model’s performance.</p> <pre># same architecture as in paper<br /># reducedim_ratio = 0.25 , same as 1/4<br />    <br />class SqueezeExcitation(nn.Module):<br />    def __init__(self, in_channels, reduced_dim):<br />        super(SqueezeExcitation, self).__init__()<br />        self.se = nn.Sequential(<br />            nn.AdaptiveAvgPool3d(1),    # input C x H x W --&gt; C x 1 X 1  ONE value of each channel<br />            nn.Conv3d(in_channels, reduced_dim, kernel_size=1), # expansion<br />            nn.SiLU(), # activation<br />            nn.Conv3d(reduced_dim, in_channels, kernel_size=1), # brings it back<br />            nn.Sigmoid(),<br />        )<br />    <br />    def forward(self, x):<br />        return x*self.se(x)<br /></pre> <h4>The Tape</h4> <p>The Stochastic Depth mechanism operates like a form of dropout, randomly deactivating some layers but only during the training phase. This randomness adds a dynamic element to the network, making it adaptive and robust. It allows for variability in the network’s depth, ensuring that different configurations are explored during training to enhance model performance.</p> <pre>class StochasticDepth(nn.Module):<br />    def __init__(self, survival_prob=0.8):<br />        super(StochasticDepth, self).__init__()<br />        self.survival_prob =survival_prob<br />        <br />    def forward(self, x): #form of dropout , randomly remove some layers not during testing<br />        if not self.training:<br />            return x<br />        binary_tensor = torch.rand(x.shape[0], 1, 1, 1, 1, device= x.device) &lt; self.survival_prob # maybe add 1 more here<br />        return torch.div(x, self.survival_prob) * binary_tensor</pre> <h4>The MobileConv3d</h4> <p>The MBConv3d class is instrumental in our model and is responsible for various phases, including expansion, depthwise convolution, squeeze excitation, and the output phase. It introduces depth-adaptive and efficient features to the network, enhancing its capacity to learn and make accurate predictions while also considering potential downsampled situations. The Stochastic Depth mechanism is utilized to introduce randomness during training, further contributing to the model&#39;s robustness.</p> <pre>class MBConv3d(nn.Module):<br />    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, <br />                 expand_ratio = 6, <br />                 reduction = 4, #squeeze excitation 1/4 = 0.25<br />                 survival_prob =0.8 # for stocastic depth<br />                 ):<br />        super(MBConv3d, self).__init__()<br />        <br />        self.survival_prob = 0.8<br />        self.use_residual = in_channels == out_channels and stride == 1 # Important if we downsample then we can&#39;t use skip connections<br />        hidden_dim = int(in_channels * expand_ratio)<br />        self.expand = in_channels != hidden_dim # every first layer in MBConv<br />        reduced_dim = int(in_channels/reduction)<br />        self.padding = padding<br />        <br />        ##expansion phase<br /><br />        self.expand = nn.Identity() if (expand_ratio == 1) else CNNBlock3d(in_channels, hidden_dim, kernel_size = 1)<br />        <br />        ##Depthwise convolution phase<br />        self.depthwise_conv = CNNBlock3d(hidden_dim, hidden_dim,<br />                                        kernel_size = kernel_size, stride = stride, <br />                                        padding = padding, groups = hidden_dim<br />                                       )<br />        <br />        # Squeeze Excitation phase<br />        self.se = SqueezeExcitation(hidden_dim, reduced_dim = reduced_dim)<br />        <br />        #output phase<br />        self.pointwise_conv = CNNBlock3d(hidden_dim, out_channels, kernel_size = 1, stride = 1, act = False, padding = 0)<br />        # add Sigmoid Activation as mentioned in the paper<br />        <br />        # drop connect<br />        self.drop_layers = StochasticDepth(survival_prob = survival_prob)<br /><br />    <br />    def forward(self, x):<br />        <br />        residual = x<br />        x = self.expand(x)<br />        x = self.depthwise_conv(x)<br />        x = self.se(x)<br />        x = self.pointwise_conv(x)<br />        <br />        if self.use_residual:  #and self.depthwise_conv.stride[0] == 1:<br />            x = self.drop_layers(x)<br />            x += residual<br />        return x</pre> <blockquote><strong>Expansion Phase:</strong><br/> In this phase, the network aims to increase the channel dimensions of the input features.<br/> Expansion helps create a richer representation by transforming low-dimensional feature maps into higher-dimensional ones.<br/> It typically employs 1x1 convolutions to increase the number of channels.</blockquote> <blockquote><strong>Depthwise Convolution Phase:</strong><br/> This phase focuses on the spatial dimension of the features, aiming to capture local patterns.<br/> Depthwise convolutions apply a separate convolutional filter to each input channel, which helps reduce the computational cost.<br/> They are particularly useful for detecting spatial features within the data.</blockquote> <blockquote><strong>Squeeze Excitation Phase:</strong><br/> This phase introduces attention mechanisms to assign importance to different channels.<br/> It involves two steps: squeezing and exciting.<br/> The “squeezing” step computes the global statistics for each channel, often through global average pooling.<br/> The “exciting” step learns how to re-weight these channels, emphasizing or de-emphasizing them based on their importance.<br/> Squeeze Excitation enhances the network’s capability to focus on relevant features.</blockquote> <blockquote><strong>Output Phase:</strong><br/> After feature extraction, the output phase involves 1x1 convolutions to reduce the number of channels and prepare the features for the final classification or regression tasks.<br/> Optionally, activation functions like Sigmoid may be applied to the output to ensure that the network’s predictions fall within a suitable range, such as [0, 1] for binary classification tasks.</blockquote> <h4>Put all the Pieces together</h4> <pre>class EfficeientNet3d(nn.Module):<br />    def __init__(self, width_mult=1, depth_mult=1, dropout_rate=0.1, num_classes=2):<br />        super(EfficeientNet3d, self).__init__()<br />        last_channels = ceil(512 * width_mult)<br /><br />        self.first_layer = CNNBlock3d(1, 64, kernel_size=7, stride=2, padding=3)<br />        self.pool = nn.MaxPool3d(1, stride=2)<br />        self.features = self._feature_extractor(width_mult, depth_mult, last_channels)<br />        self.classifier = nn.Sequential(<br />            nn.Dropout(dropout_rate),<br />            nn.Linear(last_channels * 3 * 4 * 3, 400),<br />            nn.Linear(400, 64),<br />            nn.Linear(64, num_classes),  # Adjust the output size based on the number of classes<br />        )<br /><br />    def _feature_extractor(self, width_mult, depth_mult, last_channel):<br />        # Your previous code for scaling channels and layers<br /><br />        layers = []<br />        in_channels = 64  # Initial input channels after the first layer<br />        final_in_channel = 0 #Initialzse<br /><br />        # Define configurations for the custom MBConv blocks<br />        mbconv_configurations = [<br />            (3, 1, 64, 64, 1),<br />            (5, 2, 64, 96, 1),<br />            (5, 2, 96, 128, 2),<br />            (5, 2, 128, 192, 3),<br />            (3, 1, 192, 256, 1),<br />        ]<br /><br />        for kernel_size, stride, in_channels, out_channels, repeats in mbconv_configurations:<br />            layers += [<br />                MBConv3d(in_channels if repeat == 0 else out_channels,<br />                         out_channels,<br />                         kernel_size=kernel_size,<br />                         stride=stride if repeat == 0 else 1,<br />                         expand_ratio=1,  # Assuming you want expansion factor 1 for these blocks<br />                         padding=kernel_size // 2<br />                         )<br />                for repeat in range(repeats)<br />            ]<br />            final_in_channel = out_channels<br />            print(f&#39;in_channels : {in_channels}, out_channels: {out_channels}, kernelsize : {kernel_size}, stride: {stride}, repeats: {repeats}&#39;)<br />#         print(f&#39;final_in_channels : {final_in_channel}&#39;)    <br />        layers.append(MBConv3d(final_in_channel, last_channel, kernel_size=1, stride=1, padding=0))<br />        return nn.Sequential(*layers)<br /><br />    def forward(self, inputs):<br />        out = self.first_layer(inputs)<br />        out = self.pool(out)<br />        x = self.features(out)<br />        dummy = x.view(x.shape[0], -1)<br />        out = self.classifier(dummy)<br />        return out</pre> <p>The “EfficeientNet3d” class comprises several crucial components. It starts with “first_layer,” which efficiently extracts fundamental features from the input 3D images. Subsequently, a pooling operation is applied to further refine the data.</p> <p>The heart of this architecture lies in the “features” section, where a sequence of custom-designed MBConv3d blocks comes into play. These blocks expertly manage channel expansion, depthwise convolutions, and squeeze excitation, allowing the network to capture intricate details within the medical images effectively. The hyper parameters are in accordance with the paper</p> <p>The “EfficeientNet3d” model also includes a “classifier” section for interpreting the extracted features and producing meaningful predictions. This classifier can be configured to suit the specific classification problem, whether it involves gender classification, brain age detection, or brain age deficit prediction.</p> <p>With this versatile <strong>EfficeientNet3d</strong> model, you have the flexibility to tackle a wide range of tasks that align with your specific use case. Whether it’s gender classification, brain age detection, or brain age deficit prediction, the architecture can be adapted and fine-tuned to address your unique requirements. Its adaptability makes it a powerful tool that can be customized for a variety of medical imaging and analysis tasks, providing you with the means to achieve your goals effectively.</p> <p>You can access my presentation of the implementation through the provided link <a href="https://www.linkedin.com/in/ninad-aithal/overlay/projects/17032296/multiple-media-viewer?profileId=ACoAACS7NC8BHGZiEVgGI-MlITb8IwTlAv9ohPE&amp;treasuryMediaId=1635543477926&amp;type=DOCUMENT&amp;lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base%3BbS%2FFP1oXQdmJW2Jxjz8tsg%3D%3D">here</a>, The complete code is available in my <a href="https://www.kaggle.com/code/ninadaithal6/final-3d-efficientnet/">kaggle notebook</a>. I want to highlight that the Gender classification, Brain Age detection, and Brain Age Deficit Prediction have already been successfully executed. If you require any assistance or have questions, feel free to reach out to me at <a href="mailto:reachninadaithal@gmail.com">reachninadaithal@gmail.com</a>. I’m here to help and provide support for any inquiries or further insights.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=95183f1e4580" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">MRI preprocessing using FSL</title><link href="https://blackpearl006.github.io/blog/2023/mri-preprocessing-using-fsl/" rel="alternate" type="text/html" title="MRI preprocessing using FSL"/><published>2023-09-05T05:19:19+00:00</published><updated>2023-09-05T05:19:19+00:00</updated><id>https://blackpearl006.github.io/blog/2023/mri-preprocessing-using-fsl</id><content type="html" xml:base="https://blackpearl006.github.io/blog/2023/mri-preprocessing-using-fsl/"><![CDATA[<p>This tutorial should help you to work your way through FSL</p> <p>For this article / tutorial i have used the a MRI image from OASIS dataset which you can access using this link <a href="https://github.com/blackpearl006/MRI-analysis-using-FSL">🔗</a></p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/960/1*rlrgg0lC8e1rMNGUdP5G1A.gif"/><figcaption>Using 3d slicer visualize the MRI scan</figcaption></figure> <p>Extract information about the MRI image</p> <pre>fslinfo mri_image.nii.gz</pre> <p>the output of the above command gives a lot of information about the MRI image file and not about the subject</p> <p>dim1 128 dim2 256 dim3 256 shows that this is a 128x256x256 matrix <br/>the output also mentions that each voxel is 1.25 mm x 1 mm x 1 mm</p> <p>FSL assumes that the brain is made up of small volumes and this can be visualized below</p> <iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fsketchfab.com%2Fmodels%2Fccdcbe54a5ae490e8a4f8073838f7f9a%2Fembed&amp;display_name=Sketchfab&amp;url=https%3A%2F%2Fsketchfab.com%2F3d-models%2Fvoxel-brain-ccdcbe54a5ae490e8a4f8073838f7f9a&amp;image=https%3A%2F%2Fmedia.sketchfab.com%2Fmodels%2Fccdcbe54a5ae490e8a4f8073838f7f9a%2Fthumbnails%2F5f4baf7757474c68a9d1182d4df253bb%2F107f48468f944cd5aa1ba06884f8d302.jpeg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=sketchfab" width="640" height="360" frameborder="0" scrolling="no"><a href="https://medium.com/media/cecb9383fa089362a1eb402a40e59aa6/href">https://medium.com/media/cecb9383fa089362a1eb402a40e59aa6/href</a></iframe> <h3>Skull Stripping</h3> <p>we are going remove the skull non tissue part of the brain</p> <pre>bet2 mri_image.nii.gz skull_stripped -f 0.5</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/960/1*3AKz1OwMFQiGrhXrg7BQxA.gif"/><figcaption>skull stripped MRI image</figcaption></figure> <p>bet2 has a important parameter Fractional Intensity Parameter (f) which is set to a default value of 0.5, smaller values of f give larger brain outline estimates.</p> <blockquote>Assignment : Try using various different values of F and see what happens</blockquote> <h3>Field of view</h3> <p>Not all the part of the MRI image is important, we can discard the lower head and neck part of the MRI image so that we can focus more on the brain, we use a tool roboustfov</p> <pre>robustfov -v -i skull_stripped.nii.gz -r roiimage</pre> <blockquote>Final FOV is: <br/>0.000000 128.000000 0.000000 256.000000 85.000000 170.000000 <br/>Xmin Xmax Ymin Ymax Zmin Zmax</blockquote> <pre>fslinfo roiimage.nii.gz</pre> <blockquote>MRI image is now of the shape 128x256x170</blockquote> <h3>Reorientation</h3> <p>Sometimes the MRI images are not in the standard orientation like they can be flipped, mirrored , then we can run fslreorient2std this is not a registration tool and can perform only 90°, 180°, 270° rotations</p> <pre>fslreorient2std roiimage.nii.gz<br />#to create a new output file with the orienattion of the standard MNI template<br />fslreorient2std roiimage.nii.gz reoriented</pre> <blockquote>1 0 0 0<br/>0 1 0 0<br/>0 0 1 0<br/>0 0 0 1</blockquote> <p>the output is a identity matrix, implying that the MRI image in the correct orientation ie,</p> <ul><li>x axis orient Right-to-Left</li><li>y axis orient Posterior-to-Anterior</li><li>z axis orient Inferior-to-Superior</li></ul> <h3>Brain Segmentation</h3> <p>now we have a skull stripped brain image and we will segment the brain into various parts, normally White matter, Grey Matter &amp; Cerebrospinal fluid (CSF) in case of T1 weighted MRI images, if there are big lesions in the brain , we can classify them into another class</p> <p>we use FAST tool to achieve brain and along with segmentation it also corrects the bias in the the MRI images. FAST uses Mori-Tanaka approach to classify the voxel into a class and each voxel can belong to different classes, the probability of it belonging to a class is showed in it’s intensity</p> <blockquote>Assignment: Find out what is Bias in the MRI images, Is it Spatially invariant or not.</blockquote> <p>HINT : check -b option in fast</p> <p>FAST is a iterative tool and there is a trade off between number of iterations (time taken) and the accuracy of the segmentation.</p> <pre>fast -o fast_out -b -B -t 1 --iter=10 -v roiimage.nii.gz </pre> <p>output :<br/>.<br/>├── fast_out_bias.nii.gz <br/>├── fast_out_mixeltype.nii.gz<br/>├── fast_out_pve_0.nii.gz<br/>├── fast_out_pve_1.nii.gz<br/>├── fast_out_pve_2.nii.gz<br/>├── fast_out_pveseg.nii.gz<br/>├── fast_out_restore.nii.gz<br/>├── fast_out_seg.nii.gz<br/>├── mri_image.nii.gz<br/>├── reoriented.nii.gz<br/>├── roiimage.nii.gz<br/>└── skull_stripped.nii.gz</p> <p><strong>Partial volume maps </strong>(pve) , for each class, where each voxel contains a value in the range 0–1 that represents the proportion of that class’s tissue present in that voxel.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/960/1*ku-jqefVQoaROlf2NUAmTw.gif"/><figcaption>partial volume maps for class0 (CNF), class1 (Grey matter) and class2 (White Matter)</figcaption></figure> <p><strong>Restored input</strong> is the estimated restored input image after correction for bias field.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/960/1*EDF1GFsQk4YuipO2XUl3WA.gif"/><figcaption>this is the bias corrected output image</figcaption></figure> <p><strong>Bias field</strong> is the estimated bias field.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*9i9VQjHqgn1DkeUnQbuwXA.gif"/><figcaption>bias is not spatially invariant</figcaption></figure> <p>Fast Segmented images ,, has all the classes in a single MRI image</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/960/1*bVVmVQLD4FsKNoT-z95clA.gif"/><figcaption>Segmented MRI image</figcaption></figure> <p>The brain segmentation is very useful in Volumetric caluclations, which we wont look into now,</p> <h3>Template registration</h3> <p>We want the same cartesian / voxel co-ordinates to point us at the same anatomical structure ie, align the MRI image.</p> <p><strong>Why do we do so ?? Isn’t this tampering with the MRI data</strong></p> <ol><li>For combining / comparing across various groups of people</li><li>Quantify structural changes</li><li>correcting motion artifacts in fMRI study</li></ol> <figure><img alt="" src="https://cdn-images-1.medium.com/max/935/1*yo4JHgCOHPPmZoIw9k-tOA.png"/><figcaption>same location points to different anatomical structures, image only for illustration</figcaption></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/650/1*A7Yy1sEbaxhr2mtVfW3X3A.png"/><figcaption>voxel location = anatomical location</figcaption></figure> <p>Types of transformations</p> <ol><li>Rigid body (6 DOF) used normally within-subject motion<br/>3 Rotations, 3 Translations (All the possible motions without changing the shape / size)</li><li>Non-linear (lots of DOF!) has high-quality image and works better with a non-linear template (e.g. MNI152_TI_2mm)<br/>Can be specific to the region and match the tempalate</li><li>Affine (12 DOF) needed as a starting point for non-linear align to affine template, or using lower quality images, or eddy current correction<br/>Along with the Rigid Body transformations it has 3 scaling and 3 skews</li><li>Global scaling (7 DOF) within-subject but with global scaling (equal in x,y,z) corrects for scanner scaling drift in longitudinal studies</li></ol> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*g_ltmmMR93RH_qqZTC6DIg.png"/></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*fOoxPCRbn0xARFYq1quYxA.png"/><figcaption>Left : Bias corrected, skull stripped MRI input image Right: MNI158 template</figcaption></figure> <h4>Linear transformation</h4> <pre>flirt -in fast_out_restore.nii.gz -ref /Users/ninad/fsl/data/standard/MNI152_T1_2mm.nii.gz -dof 12 -omat MRI_to_MNI.mat -out affine_trans</pre> <p>the above command does the affine transformation (12 DOF) to registers the MRI image to MNI158 standard 2mm template and furture help in the non linear transformation. -in the input MRI image path, -ref reference template (we used MNI 2mm template) -dof specifies DOF and i wanted Affine transformation so set it to 12, -omat outputs a 4x4 matrix in a .mat file with the affine transformation values, -out outputs the MRI image after registering to the reference image</p> <pre>cat MRI_to_MNI.mat</pre> <blockquote>| 0.04992739769 -0.009461175045 1.376357676 -31.28157938 |<br/>| -1.610147028 -0.1469847473 0.03976417704 252.6235512 |<br/>| 0.182484106 -1.599149772 0.1794447228 206.7580262 |<br/>| 0 0 0 1 |</blockquote> <figure><img alt="" src="https://cdn-images-1.medium.com/max/960/1*65HTU0VJoOVtOvuny1XhoQ.gif"/><figcaption>MNI158 registered template</figcaption></figure> <p>The affine transformation can result in various changes like change in axis , for the visualization you can use the</p> <pre>fslreorient2std affine_trans.nii.gz reoriented_affine_tranform</pre> <p>There are many cost functions available in FLIRT, within-modality functions Least Squares and Normalised Correlation, as well as the between-modality functions Correlation Ratio (the default), Mutual Information and Normalised Mutual Information.<br/>within-modality means the reference and the input MRI image should be of same type</p> <h4>Non linear Transformation</h4> <ul><li>Bending energy regularization penalizes deformations that involve bending or warping of the image too much. It encourages smoother and more gradual deformations.</li><li>This regularization is typically used when you want to avoid sharp and unrealistic deformations in the transformation field. It is especially useful when the deformation should be smooth and gradual, such as in medical image registrationFnirt is a command line program that is run by typing fnirt followed by some set of parameters. The minimum you need to type is</li></ul> <pre>fnirt --in=affine_trans.nii.gz --ref=/Users/ninad/fsl/data/standard/MNI152_T1_2mm  -v</pre> <p>but it is not very likely it will do you any good. Fnirt has a large set of parameters that determine what is done, and how it is done. Without a knowledge of these parameters you will not get the best results that you can.</p> <p>Efficiently using FNIRT</p> <pre>fnirt --in=roiimage.nii.gz --ref=/Users/ninad/fsl/data/standard/MNI152_T1_2mm.nii.gz --aff=MRI_to_MNI.mat --refmask=/Users/ninad/fsl/data/standard/MNI152_T1_2mm_brain_mask_dil.nii --config=/Users/ninad/fsl/etc/flirtsch/T1_2_MNI152_2mm.cnf --cout=fnirt_coef_warp --iout=fnirt_warped --fout=fnirt_warp --jout=fnirt_jacobian --refout=fnirt_ref_intensity --intout=fnirt_intensity_modulation -v --logout=fnirt_log</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/960/1*kT8xzOOJfPkrCtqHasKSSA.gif"/><figcaption>FNIRT output</figcaption></figure> <p>Is this what we were looking for ??<br/>Comment below what went wrong…..? 🤔</p> <blockquote>Assignment : check out the config file located at <em>fsl/etc/flirtsch/T1_2_MNI152_2mm.cnf</em> and compare it with the options defined for the fnirt tool using — help option<br/>Future work : Try to run fsl using R programming language using any fsl wrapper <a href="https://rdrr.io/github/neuroconductor-devel-releases/fslr/man/download_fsl.html">packages</a> <br/>Notes : always use -v (verbose) option in your commands to know what’s happening</blockquote> <p>References : <a href="https://youtu.be/2zcfYgdxhKM?si=cZxzpJqOPKjCp7QB">FSL Course</a>, <a href="https://ggooo.wordpress.com/2014/07/31/inverse-transformation-using-flirt-2/">Random code</a>, <a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FNIRT/UserGuide">FSL user guide</a></p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=383a67e7185" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">What makes the analysis of MRI data challenging?</title><link href="https://blackpearl006.github.io/blog/2023/what-makes-the-analysis-of-mri-data-challenging/" rel="alternate" type="text/html" title="What makes the analysis of MRI data challenging?"/><published>2023-08-21T17:55:03+00:00</published><updated>2023-08-21T17:55:03+00:00</updated><id>https://blackpearl006.github.io/blog/2023/what-makes-the-analysis-of-mri-data-challenging</id><content type="html" xml:base="https://blackpearl006.github.io/blog/2023/what-makes-the-analysis-of-mri-data-challenging/"><![CDATA[<p>After spending a good amount creating deep neural models on Human brain MRI images. I’m here, letting you know <em>What makes working with MRI data so tough ?</em></p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/796/0*le5iJ_GoQVi5d2Sy"/><figcaption>A sample Brain MRI from OASIS dataset</figcaption></figure> <p>Listed below are the some of the reasons, which in my opinion makes MRI analysis hard</p> <ol><li><strong>Data Size<br/></strong>MRI data can be very large in size and handling and working with a large amount of MRI scans can be very difficult to manage</li><li><strong>Different medical imaging modalities<br/></strong>CT, MRI, PET all have varying principles, settings, and capabilities resulting in capturing distinct dimensions and features in the brain images.</li><li><strong>Preprocessing the MRI image<br/></strong>During the process of skull stripping using the bet2 tool from FSL, A significant portion of the brain was also removed leading to corruption in the dataset</li><li><strong>Imbalance in Medical condition</strong><br/>People suffering from a particular medical condition, will be smaller when compared to the whole human population</li><li><strong>Skilled workers to get MRI</strong><br/>conducting MRI scan requires high level of expertise and thus limits accessibility</li></ol> <p>Let me know what you think @ <a href="https://www.linkedin.com/in/ninad-aithal/">LinkdIn</a></p> <p>Reference : <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3312396/">Brain Imaging in Alzheimer Disease</a></p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=37fe5b5a6ec6" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry></feed>