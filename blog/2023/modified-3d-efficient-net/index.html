<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>This blog is the implementation of the <a href="https://ietresearch.onlinelibrary.wiley.com/doi/pdf/10.1049/ipr2.12618" rel="external nofollow noopener" target="_blank">paper</a></p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*fP200xCoAkl_suYM"><figcaption>Photo by <a href="https://unsplash.com/@v2osk?utm_source=medium&amp;utm_medium=referral" rel="external nofollow noopener" target="_blank">v2osk</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="external nofollow noopener" target="_blank">Unsplash</a></figcaption></figure> <p>Efficient Net was introduced not through the crucible of the Image Net competition but in a paper titled “<a href="https://arxiv.org/pdf/1905.11946.pdf" rel="external nofollow noopener" target="_blank">Efficient Net: Rethinking Model Scaling for Convolutional Neural Networks,</a>” authored by Tan and Le in 2019. This groundbreaking work posed a fundamental question: “<strong>Can we create a model with significantly fewer parameters than the well-established deep CNN's like Res Net and VGG Net, yet achieve comparable levels of accuracy?</strong>” The answer, as it turned out, was a resounding “<strong>yes</strong>.” However, Efficient Net’s innovation wasn’t solely about reducing parameters; it sought an optimal equilibrium between model depth, width, and resolution. This was achieved through the introduction of a compound scaling method that enabled the creation of efficient yet highly accurate models.</p> <p>In this blog, Using a Modified 3D Efficient Net on Registered and Skull Stripped OASIS MRI images to classify the subjects into Healthy and MCI, and extending the same model to perform Gender classification, Brain Age detection and Brain Age Deficit Prediction using a Double headed model (One Regression and one classification head)</p> <p>OASIS is a publicly available and can be access at <a href="https://www.oasis-brains.org/" rel="external nofollow noopener" target="_blank">link</a>. Raw MRI files is not suitable to be used directly in any analysis as each voxel in the MRI image in different subjects will point to different parts of the brain and non-brain matter which do not have any significance might hinder the model’s capability to learn.Keeping all this in mind I used a custom made pipeline including Brain extraction , bias correction and MNI template registering using FSL and the preprocessed MRI images can be found at the link below :</p> <p><a href="https://github.com/blackpearl006/OASIS_MNI_Registered" rel="external nofollow noopener" target="_blank">GitHub - blackpearl006/OASIS_MNI_Registered</a></p> <p>To know more about the preprocessing steps in detail , read my Blog</p> <p><a href="https://medium.com/@daminininad/mri-preprocessing-using-fsl-383a67e7185" rel="external nofollow noopener" target="_blank">MRI preprocessing using FSL</a></p> <p>I’ve created a custom dataset class designed to handle MRI images stored in the NifTi format using the NiBabel library. This class loads these images as numpy arrays for easy data manipulation. Additionally, it performs a crucial data preprocessing step: normalization, ensuring that the data is consistently scaled for further analysis.</p> <p>Moreover, this dataset class takes on the responsibility of labeling each subject. These labels are assigned based on a key clinical metric: the Clinical Dementia Rating (CDR) value. The resulting labels categorize subjects into two meaningful groups: Non-Demented individuals and those with a diagnosis of Probable Alzheimer’s Disease (AD).</p> <pre>class CustomDataset():<br>    def __init__(self, data_dir, csv_path, num_samples = 200, transform = None):  <br>        self.filelabels = self.load_class_labels(csv_path)<br>        self.file_paths = self.get_file_paths(data_dir)<br>        self.transform = transform<br>        <br>    def __len__(self):<br>        return len(self.file_paths)<br>    <br>    def __getitem__(self, idx):<br>        file_path = self.file_paths[idx]<br>        nifti_data = nib.load(file_path)<br>        data = nifti_data.get_fdata()<br>        preprocessed_data = self.preprocess_data(data)<br>        <br>        preprocessed_tensor = torch.tensor(preprocessed_data, dtype=torch.float32).unsqueeze(0)<br><br>        file_id = file_path.split('/')[-1][0:13]<br>        if self.filelabels is not None:<br>            label = torch.tensor(self.filelabels[file_id], dtype=torch.long)<br>            return preprocessed_tensor, label<br>        else:<br>            return preprocessed_tensor<br>                    <br>    def get_file_paths(self, path):<br>        file_paths = []<br>        scans = os.listdir(path)<br>        scans = [scan for scan in scans if scan[:4] == 'OAS1']<br>        for folder_name in scans:<br>            folder_path = os.path.join(path, folder_name)  # Corrected variable name here<br>            if os.path.isdir(folder_path) :<br>                for file_name in os.listdir(folder_path):<br>                    file_path = os.path.join(folder_path, file_name)<br>                    label = self.filelabels[file_path.split('/')[-1][0:13]]<br>                    file_paths.append(file_path)<br>        return file_paths<br>    <br>    <br>    def load_class_labels(self, csv_path):<br>        df = pd.read_csv(csv_path)<br>        class_labels = {}<br>        for _, row in df.iterrows():<br>            id_value = row['ID']<br>            cdr_value = row['CDR']<br>            class_labels[id_value] = 0 if pd.isna(cdr_value) or float(cdr_value) == 0.0 else 1<br>        return class_labels<br>    <br>    def preprocess_data(self, data):<br>        mean = data.mean()<br>        std = data.std()<br>        normalized_data = (data - mean) / std<br>        return normalized_data<br><br><br>csv_path = 'path-to-metadata-file/kaggle.csv'<br>data_dir = 'path/to/downloaded/OASIS_MNI_Registered'<br>custom_dataset = CustomDataset(data_dir, csv_path)</pre> <p>I’ve divided the dataset into training, validation, and test sets, allocating 70% for training, 20% for validation, and 10% for testing. During model training, I’ve chosen a batch size of 32. This approach helps strike a balance between computational efficiency and effective model training.</p> <h3>The Efficient Net Architecture</h3> <h4>Fundamental Building Block</h4> <p>At the core of this architecture lies a fundamental building block, a straightforward CNN layer enhanced with batch normalization and activated using the SiLU (Sigmoid-weighted Linear Unit) or Swish activation function. This building block forms the basis for our entire implementation.</p> <pre>class CNNBlock3d(nn.Module):<br>    def __init__(self, in_channels, out_channels, kernel_size, stride = 1, padding = 0 , groups=1, act=True, bn=True, bias=False):<br>        super(CNNBlock3d, self).__init__()<br>        self.cnn = nn.Conv3d(in_channels, out_channels, kernel_size, stride, padding, groups=groups, bias=bias) #bias set to False as we are using BatchNorm<br>        <br>        # if groups = in_channels then it is for Depth wise convolutional; For each channel different Convolutional kernel<br>        # very limited change in loss but a very high decrease in number of paramteres<br>        # if groups = 1 : normal_conv kernel of size kernel_size**3<br><br>        self.bn = nn.BatchNorm3d(out_channels) if bn else nn.Identity() <br>        self.silu = nn.SiLU() if act else nn.Identity() ##SiLU &lt;--&gt; Swish same Thing<br>        # 1 layer in MBConv doesn't have activation function<br>    <br>    def forward(self, x):<br>        out = self.cnn(x)<br>        out = self.bn(out)<br>        out = self.silu(out)<br>        return out<br>    <br># return self.silu(self.bn(self.cnn(x)))</pre> <p>The SiLU, also known as the Swish activation function, implemented in our network exhibits a unique characteristic. It permits a small amount of negative gradient to flow within the network, in contrast to the widely popular ReLU activation function, which restricts negative gradient flow. Additionally, although the paper does not explicitly detail batch normalization, we have incorporated it into our model. This addition simplifies the learning process for the model, enhancing its capabilities.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/842/0*WcfMo1jbm7g9Od5X.png"><figcaption>Difference between ReLU and SiLU (Swish)</figcaption></figure> <h4>The Ranking master</h4> <p>The squeeze excitation layer is used to <strong>compute attention score for each channels,</strong> effectively assigning a value to each channel based on its importance. This process guides us in determining how much emphasis we should place on each channel. Subsequently, the original feature map is then scaled with these channel-specific attention scores, optimizing our model’s performance.</p> <pre># same architecture as in paper<br># reducedim_ratio = 0.25 , same as 1/4<br>    <br>class SqueezeExcitation(nn.Module):<br>    def __init__(self, in_channels, reduced_dim):<br>        super(SqueezeExcitation, self).__init__()<br>        self.se = nn.Sequential(<br>            nn.AdaptiveAvgPool3d(1),    # input C x H x W --&gt; C x 1 X 1  ONE value of each channel<br>            nn.Conv3d(in_channels, reduced_dim, kernel_size=1), # expansion<br>            nn.SiLU(), # activation<br>            nn.Conv3d(reduced_dim, in_channels, kernel_size=1), # brings it back<br>            nn.Sigmoid(),<br>        )<br>    <br>    def forward(self, x):<br>        return x*self.se(x)<br></pre> <h4>The Tape</h4> <p>The Stochastic Depth mechanism operates like a form of dropout, randomly deactivating some layers but only during the training phase. This randomness adds a dynamic element to the network, making it adaptive and robust. It allows for variability in the network’s depth, ensuring that different configurations are explored during training to enhance model performance.</p> <pre>class StochasticDepth(nn.Module):<br>    def __init__(self, survival_prob=0.8):<br>        super(StochasticDepth, self).__init__()<br>        self.survival_prob =survival_prob<br>        <br>    def forward(self, x): #form of dropout , randomly remove some layers not during testing<br>        if not self.training:<br>            return x<br>        binary_tensor = torch.rand(x.shape[0], 1, 1, 1, 1, device= x.device) &lt; self.survival_prob # maybe add 1 more here<br>        return torch.div(x, self.survival_prob) * binary_tensor</pre> <h4>The MobileConv3d</h4> <p>The MBConv3d class is instrumental in our model and is responsible for various phases, including expansion, depthwise convolution, squeeze excitation, and the output phase. It introduces depth-adaptive and efficient features to the network, enhancing its capacity to learn and make accurate predictions while also considering potential downsampled situations. The Stochastic Depth mechanism is utilized to introduce randomness during training, further contributing to the model's robustness.</p> <pre>class MBConv3d(nn.Module):<br>    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, <br>                 expand_ratio = 6, <br>                 reduction = 4, #squeeze excitation 1/4 = 0.25<br>                 survival_prob =0.8 # for stocastic depth<br>                 ):<br>        super(MBConv3d, self).__init__()<br>        <br>        self.survival_prob = 0.8<br>        self.use_residual = in_channels == out_channels and stride == 1 # Important if we downsample then we can't use skip connections<br>        hidden_dim = int(in_channels * expand_ratio)<br>        self.expand = in_channels != hidden_dim # every first layer in MBConv<br>        reduced_dim = int(in_channels/reduction)<br>        self.padding = padding<br>        <br>        ##expansion phase<br><br>        self.expand = nn.Identity() if (expand_ratio == 1) else CNNBlock3d(in_channels, hidden_dim, kernel_size = 1)<br>        <br>        ##Depthwise convolution phase<br>        self.depthwise_conv = CNNBlock3d(hidden_dim, hidden_dim,<br>                                        kernel_size = kernel_size, stride = stride, <br>                                        padding = padding, groups = hidden_dim<br>                                       )<br>        <br>        # Squeeze Excitation phase<br>        self.se = SqueezeExcitation(hidden_dim, reduced_dim = reduced_dim)<br>        <br>        #output phase<br>        self.pointwise_conv = CNNBlock3d(hidden_dim, out_channels, kernel_size = 1, stride = 1, act = False, padding = 0)<br>        # add Sigmoid Activation as mentioned in the paper<br>        <br>        # drop connect<br>        self.drop_layers = StochasticDepth(survival_prob = survival_prob)<br><br>    <br>    def forward(self, x):<br>        <br>        residual = x<br>        x = self.expand(x)<br>        x = self.depthwise_conv(x)<br>        x = self.se(x)<br>        x = self.pointwise_conv(x)<br>        <br>        if self.use_residual:  #and self.depthwise_conv.stride[0] == 1:<br>            x = self.drop_layers(x)<br>            x += residual<br>        return x</pre> <blockquote> <strong>Expansion Phase:</strong><br> In this phase, the network aims to increase the channel dimensions of the input features.<br> Expansion helps create a richer representation by transforming low-dimensional feature maps into higher-dimensional ones.<br> It typically employs 1x1 convolutions to increase the number of channels.</blockquote> <blockquote> <strong>Depthwise Convolution Phase:</strong><br> This phase focuses on the spatial dimension of the features, aiming to capture local patterns.<br> Depthwise convolutions apply a separate convolutional filter to each input channel, which helps reduce the computational cost.<br> They are particularly useful for detecting spatial features within the data.</blockquote> <blockquote> <strong>Squeeze Excitation Phase:</strong><br> This phase introduces attention mechanisms to assign importance to different channels.<br> It involves two steps: squeezing and exciting.<br> The “squeezing” step computes the global statistics for each channel, often through global average pooling.<br> The “exciting” step learns how to re-weight these channels, emphasizing or de-emphasizing them based on their importance.<br> Squeeze Excitation enhances the network’s capability to focus on relevant features.</blockquote> <blockquote> <strong>Output Phase:</strong><br> After feature extraction, the output phase involves 1x1 convolutions to reduce the number of channels and prepare the features for the final classification or regression tasks.<br> Optionally, activation functions like Sigmoid may be applied to the output to ensure that the network’s predictions fall within a suitable range, such as [0, 1] for binary classification tasks.</blockquote> <h4>Put all the Pieces together</h4> <pre>class EfficeientNet3d(nn.Module):<br>    def __init__(self, width_mult=1, depth_mult=1, dropout_rate=0.1, num_classes=2):<br>        super(EfficeientNet3d, self).__init__()<br>        last_channels = ceil(512 * width_mult)<br><br>        self.first_layer = CNNBlock3d(1, 64, kernel_size=7, stride=2, padding=3)<br>        self.pool = nn.MaxPool3d(1, stride=2)<br>        self.features = self._feature_extractor(width_mult, depth_mult, last_channels)<br>        self.classifier = nn.Sequential(<br>            nn.Dropout(dropout_rate),<br>            nn.Linear(last_channels * 3 * 4 * 3, 400),<br>            nn.Linear(400, 64),<br>            nn.Linear(64, num_classes),  # Adjust the output size based on the number of classes<br>        )<br><br>    def _feature_extractor(self, width_mult, depth_mult, last_channel):<br>        # Your previous code for scaling channels and layers<br><br>        layers = []<br>        in_channels = 64  # Initial input channels after the first layer<br>        final_in_channel = 0 #Initialzse<br><br>        # Define configurations for the custom MBConv blocks<br>        mbconv_configurations = [<br>            (3, 1, 64, 64, 1),<br>            (5, 2, 64, 96, 1),<br>            (5, 2, 96, 128, 2),<br>            (5, 2, 128, 192, 3),<br>            (3, 1, 192, 256, 1),<br>        ]<br><br>        for kernel_size, stride, in_channels, out_channels, repeats in mbconv_configurations:<br>            layers += [<br>                MBConv3d(in_channels if repeat == 0 else out_channels,<br>                         out_channels,<br>                         kernel_size=kernel_size,<br>                         stride=stride if repeat == 0 else 1,<br>                         expand_ratio=1,  # Assuming you want expansion factor 1 for these blocks<br>                         padding=kernel_size // 2<br>                         )<br>                for repeat in range(repeats)<br>            ]<br>            final_in_channel = out_channels<br>            print(f'in_channels : {in_channels}, out_channels: {out_channels}, kernelsize : {kernel_size}, stride: {stride}, repeats: {repeats}')<br>#         print(f'final_in_channels : {final_in_channel}')    <br>        layers.append(MBConv3d(final_in_channel, last_channel, kernel_size=1, stride=1, padding=0))<br>        return nn.Sequential(*layers)<br><br>    def forward(self, inputs):<br>        out = self.first_layer(inputs)<br>        out = self.pool(out)<br>        x = self.features(out)<br>        dummy = x.view(x.shape[0], -1)<br>        out = self.classifier(dummy)<br>        return out</pre> <p>The “EfficeientNet3d” class comprises several crucial components. It starts with “first_layer,” which efficiently extracts fundamental features from the input 3D images. Subsequently, a pooling operation is applied to further refine the data.</p> <p>The heart of this architecture lies in the “features” section, where a sequence of custom-designed MBConv3d blocks comes into play. These blocks expertly manage channel expansion, depthwise convolutions, and squeeze excitation, allowing the network to capture intricate details within the medical images effectively. The hyper parameters are in accordance with the paper</p> <p>The “EfficeientNet3d” model also includes a “classifier” section for interpreting the extracted features and producing meaningful predictions. This classifier can be configured to suit the specific classification problem, whether it involves gender classification, brain age detection, or brain age deficit prediction.</p> <p>With this versatile <strong>EfficeientNet3d</strong> model, you have the flexibility to tackle a wide range of tasks that align with your specific use case. Whether it’s gender classification, brain age detection, or brain age deficit prediction, the architecture can be adapted and fine-tuned to address your unique requirements. Its adaptability makes it a powerful tool that can be customized for a variety of medical imaging and analysis tasks, providing you with the means to achieve your goals effectively.</p> <p>You can access my presentation of the implementation through the provided link <a href="https://www.linkedin.com/in/ninad-aithal/overlay/projects/17032296/multiple-media-viewer?profileId=ACoAACS7NC8BHGZiEVgGI-MlITb8IwTlAv9ohPE&amp;treasuryMediaId=1635543477926&amp;type=DOCUMENT&amp;lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base%3BbS%2FFP1oXQdmJW2Jxjz8tsg%3D%3D" rel="external nofollow noopener" target="_blank">here</a>, The complete code is available in my <a href="https://www.kaggle.com/code/ninadaithal6/final-3d-efficientnet/" rel="external nofollow noopener" target="_blank">kaggle notebook</a>. I want to highlight that the Gender classification, Brain Age detection, and Brain Age Deficit Prediction have already been successfully executed. If you require any assistance or have questions, feel free to reach out to me at <a href="mailto:reachninadaithal@gmail.com">reachninadaithal@gmail.com</a>. I’m here to help and provide support for any inquiries or further insights.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=95183f1e4580" width="1" height="1" alt=""></p> </body></html>